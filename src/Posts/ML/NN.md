---
title: Deep Learning An Introduction for Applied Mathematicians
# icon: material-symbols:add-notes-outline
order: 4
date: 2024-09-10
category:
  - ML

tag:
  - note
  - theorem
---

# Deep Learning: An Introduction for Applied Mathematicians

[ä¾†æº](https://arxiv.org/pdf/1801.05894)

æ³¨æ„ï¼Œè«–æ–‡ä¸­æ˜¯ç”¨ matlab å¯¦ä½œï¼Œè€Œæˆ‘æœƒä½¿ç”¨ python ä¾†å¯¦ä½œã€‚

<!-- more -->

## ä¾‹å­

![](img/NN/1.png)


æˆ‘å€‘æƒ³è¦ç”¨ä¸€æ¢ç·šåˆ†é¡ O X ï¼Œä¹‹å¾Œå°‡æœƒåˆ©ç”¨åŸºæ–¼ sigmoid function çš„ç¥ç¶“ç¶²è·¯ä¾†æ‰¾ã€‚

### sigmoid funcion

Sigmoid function é•·å¦‚ä¸‹

$$\sigma(x) = \frac{1}{1+e^{-x}}$$

å¾®åˆ†å½¢å¼

$$\sigma'(x) = \sigma(x)(1-\sigma(x))$$

å¯ä»¥æ”¹è®Šç‚ºä½ç§»ï¼Œå’Œç¸®æ”¾ï¼Œä¾‹å¦‚ $\sigma(3(x-5))$. å³ç§»åˆ° 5 ç„¶å¾Œæ›´åŠ é™¡å³­ã€‚

![](img/NN/2.png)

å¯ä»¥åˆ©ç”¨ sigmoid function è¨­å®šç²¾ç¥å…ƒå±¤ï¼Œå¦‚æœä¸€å€‹ç¥ç¶“å…ƒç‚ºä¸€å€‹å‘é‡ $a$ï¼Œå…¶è¼¸å‡ºåˆ°ä¸‹ä¸€å±¤å‰‡é•·æˆå¦‚ä¸‹

$$\sigma(Wa + b)$$

- $W$ æ˜¯ä¸€å€‹çŸ©é™£ï¼Œä»£è¡¨æ¬Šé‡
- $b$ æ˜¯ä¸€å€‹å‘é‡ï¼Œä»£è¡¨åå·®

å¯«å‡ºåˆ†é‡å‰‡æ˜¯

$$\sigma(\sum_j w_{ij}a_j + b_i)$$

ç‚ºäº†æ–¹ä¾¿ä¸æœƒç”¨åˆ†é‡ä¾†é¡¯ç¤ºã€‚

### å››å±¤ç¥ç¶“ç¶²è·¯ä¾‹å­

![](img/NN/3.png)

å¾ Layer1 åˆ° Layer2 ç”¨ä¸Šè¿° sigmoid function è¡¨ç¤ºç‚º

$$\sigma(W^{[2]}a + b^{[2]}) \in \mathbb{R}^2$$

æ­¤æ™‚ $W^{[2]} \in \mathbb{R}^{2 \times 2}$

å¾ç¬¬äºŒå±¤è¦åˆ°ç¬¬ä¸‰å±¤ï¼Œå°±æŠŠå‰›å‰›å¾—åˆ°çš„æ–°å‘é‡å†å¸¶å…¥ä¸€æ¬¡

$$\sigma(W^{[3]} \sigma(W^{[2]}a + b^{[2]}) + b^{[3]}) \in \mathbb{R}^3$$

æ­¤æ™‚ $W^{[3]} \in \mathbb{R}^{3 \times 2}$

ç¹¼çºŒåšåˆ°ç¬¬å››å±¤

$$\sigma(W^{[4]} \sigma(W^{[3]} \sigma(W^{[2]}a + b^{[2]}) + b^{[3]}) + b^{[4]}) \in \mathbb{R}^2$$

æ­¤æ™‚ $W^{[4]} \in \mathbb{R}^{2 \times 3}$

æ‰€ä»¥é€™å››å±¤å¯ä»¥å®šç¾©æˆä¸€å€‹å‡½æ•¸ $F : \mathbb{R}^2 \rightarrow \mathbb{R}^2$

$$F(x) = \sigma(W^{[4]} \sigma(W^{[3]} \sigma(W^{[2]}a + b^{[2]}) + b^{[3]}) + b^{[4]}) \in \mathbb{R}^2$$

ç¸½å…±æœ‰ 23 å€‹åƒæ•¸ï¼Œç›®æ¨™æ˜¯æ‰¾å‡ºä¸€æ¢ç·šï¼Œåˆ†é–‹ O X ï¼Œä¹Ÿå°±æ˜¯éœ€è¦å» optimizing åƒæ•¸ã€‚ ç›®æ¨™ä½¿ O é¡åˆ¥ $F(x)$ è¶¨è¿‘æ–¼ $[1, 0]^T$ ï¼Œ X é¡åˆ¥ $F(x)$ è¶¨è¿‘æ–¼ $[0, 1]^T$ 

è€Œ $F(x)$ å¯ä»¥ç°¡åŒ–æˆé€é cost functionã€‚ å®šç¾©è³‡æ–™é›†ç‚º $\{x^{\{i\}}\}_{i=1}^{10}$ ç„¶å¾Œç›®æ¨™ç‚º $y(x^{\{i\}})$ã€‚

æ‰€ä»¥

$$Cost(W^{[2]}, W^{[3]}, W^{[4]}, b^{[2]}, b^{[3]}, b^{[4]}) = \frac{1}{10} \sum_{i=1}^{10} \frac{1}{2} \lVert y(x^{\{i\}}) - F(x^{\{i\}}) \rVert_2^2$$

$\frac{1}{2}$ æ˜¯ç‚ºäº†æ–¹ä¾¿å®šç¾©çš„ (å¯ä»¥çœ‹åˆ°å¹³æ–¹å¾®åˆ†æœƒæœ‰å€‹ 2å€æ•¸ä¸‹ä¾†)ï¼Œé€šå¸¸ç¨±ç‚º quadratic cost function ç”¨æœ€ä½³åŒ–ä¾†èªªé€™ä¹Ÿæ˜¯ä¸€ç¨® objective function

è€Œå°‹æ‰¾æ¬Šé‡å’Œåå·®ä¾†åšåˆ° minimize cost function ç¨±ç‚º training ç¥ç¶“ç¶²è·¯ã€‚æ³¨æ„ï¼Œèª¿æ•´ scaling cost ä¸æœƒé€ æˆä»€éº¼å½±éŸ¿ï¼Œä¾‹å¦‚ $100Cost$ æˆ–è€… $\frac{Cost}{30}$ï¼Œæ‰€ä»¥ Cost ä¸Šçš„ $\frac{1}{10}$ï¼Œ$\frac{1}{2}$ ä¸æœƒæœ‰å½±éŸ¿ã€‚

æœ€çµ‚ç›®æ¨™é”åˆ°

![](img/NN/4.png)

æ¥ä¸‹ä¾†å°±ä¾†ä»‹ç´¹è©³ç´°çš„å…§å®¹ã€‚

## The General Set-up

### åè©è§£é‡‹

ä¸Šé¢æåˆ°çš„å››å±¤ç¥ç¶“ç¶²è·¯ä¾‹å­ï¼Œæœƒç¨± Layer1 ç‚º Input layerï¼ŒLayer4 ç‚º Output layerï¼Œä¸­é–“çš„å±¤å‰‡å«åš Hidden layerã€‚ æ¯ä¸€å±¤çš„å‘é‡è¢«ç¨±ç‚ºç¥ç¶“å…ƒ Neuronsã€‚

![](img/NN/5.png)

ç¾åœ¨ä¾†è½‰æ›æˆä¸€èˆ¬å½¢å¼ï¼Œå‡è¨­ç¸½å…±æœ‰ $L$ å±¤ç¥ç¶“ç¶²è·¯ï¼Œç¬¬ $1$ å±¤ç‚ºè¼¸å…¥å±¤ï¼Œç¬¬ $L$ å±¤ç‚ºè¼¸å‡ºå±¤ï¼Œå‡è¨­ç¬¬ $l$ å±¤ç‚º $n_l$ ç¥ç¶“å…ƒï¼Œæ•´å€‹æµç¨‹å°±æ˜¯å¾ $\mathbb{R}^{n_1} \rightarrow \mathbb{R}^{n_l}$ 

æˆ‘å€‘ç”¨ $W^{[l]} \in \mathbb{R}^{n_l \times n_{l-1}}$ å®šç¾©åœ¨ $l$ å±¤çš„æ¬Šé‡çŸ©é™£ã€‚è€Œ $w^{[l]}_{jk}$ å°±æ˜¯åœ¨ $l$ å±¤çš„ç¥ç¶“å…ƒ $j$ æ‡‰ç”¨æ–¼ç¬¬ $l-1$ å±¤ç¥ç¶“å…ƒ $k$ è¼¸å‡ºçš„æ¬Šé‡ã€‚ åŒæ¨£çš„ $b^{[l]} \in \mathbb{R}^{n_l}$ å°±æ˜¯ç¬¬ $l$ å±¤çš„åå·®ã€‚

![](img/NN/6.png)

### ä¸€èˆ¬å½¢å¼

å¦‚æœçµ¦å®šè¼¸å…¥ç‚º $x \in \mathbb{R}^{n_1}$ï¼Œç¸½å…± L å±¤ç¥ç¶“ç¶²è·¯ï¼Œå‰‡å¯ä»¥è¡¨é”ç‚ºä¸‹

$$ \begin{cases}
a^{[1]} = x\\
a^{[l]} = \sigma(W^{[l]}a^{[l-1]} +b^{[l]}), \space for \space l = 2,3,...,L\\
\end{cases}$$ 

å‡è¨­è³‡æ–™æœ‰ $N$ å€‹å‰‡è¼¸å…¥ $\{x^{\{i\}}\}_{i=1}^N$ æœƒå°æ‡‰åˆ°ä¸€å€‹ç›®æ¨™è¼¸å‡º $\{y(x^{\{i\}})\}_{i=1}^N$ï¼Œæ‰€ä»¥

$$Cost = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{2} \lVert y(x^{\{i\}}) - a^{[L]}(x^{\{i\}}) \rVert_2^2$$

åˆç¨±ç‚º Forward Process

## Stochastic Gradient

æˆ‘å€‘çš„ç›®æ¨™æ˜¯ minimize $Cost$ functionï¼Œç”¨æœ€ä¸€é–‹å§‹çš„å››å±¤ç¥ç¶“å…ƒä¾‹å­ï¼Œå‡è¨­ç‹€æ…‹æ¬Šé‡å’Œåå·®é‚£äº›åƒæ•¸ç¸½å’Œç‚º $p$ï¼Œä¹Ÿå°±æ˜¯èªª $p \in \mathbb{R}^{23}$ï¼Œæˆ–è€…ä¸€èˆ¬åŒ–ç‚º $p \in \mathbb{R}^s$ï¼Œå‰‡ $Cost: \mathbb{R}^s \rightarrow \mathbb{R}$ã€‚

### gradient descent (GD)

æˆ–ç¨±ç‚º steepest desecentï¼Œæ­¤æ–¹æ³•æ˜¯ç”¨è¿­ä»£çš„æ–¹å¼é€²è¡Œï¼Œç›®æ¨™ç‚ºè®“å…¶æ”¶æ–‚åˆ°æœ€å°åŒ– $Cost$ã€‚

å‡è¨­ç›®å‰çš„å‘é‡ç‚º $p$ï¼Œä¸‹ä¸€å€‹å‘é‡ç‚ºå¢åŠ ä¸€å€‹æ“¾å‹• $\Delta p$ï¼Œä¹Ÿå°±æ˜¯ $p + \Delta p$ æˆ‘å€‘è¦å¦‚ä½•é¸æ“‡ $\Delta p$ ä¾†ä½¿è¿­ä»£æœ‰æ•ˆæœå‘¢? 

å¦‚æœ $\Delta p$ å¾ˆå°ï¼Œå‰‡ $\lVert \Delta p \rVert$ å°åˆ°å¯ä»¥å¿½ç•¥ï¼Œæ‰€ä»¥åˆ©ç”¨æ³°å‹’å±•é–‹å¯ä»¥å¾—åˆ°

$$Cost(p+\Delta p) \approx Cost(p) + \sum_{r=1}^s \frac{\partial Cost(p)}{\partial p_r} \Delta p_r$$

æˆ–è€…ç”¨ gradient $\nabla$ ç¬¦è™Ÿä¾†è¡¨ç¤º


$$Cost(p+\Delta p) \approx Cost(p) + \nabla Cost(p)^T\Delta p$$

é€é Cauchy-Schwarz inequalityï¼Œå‡è¨­å…©å€‹ $f, g \in \mathbb{R}^s$ æˆ‘å€‘æœ‰ $|f^Tg| <= \lVert f \rVert_2 \Vert g\Vert_2$ æ‰€ä»¥æœ€è² å€¼ $|f^Tg|$ ç‚º $-\lVert f \rVert_2 \Vert g\Vert_2$ ä¸” $f = -g$ 

æ‰€ä»¥æ—¢ç„¶è¦æœ€å°åŒ–ï¼Œæˆ‘å€‘çš„ $\Delta p$ å°±è¦é¸æ“‡ $-\nabla Cost (p)$

ç„¶è€Œæˆ‘å€‘æœ‰äº†æ–¹å‘ï¼Œä½†æ˜¯å¯¦éš›ç§»å‹•çš„ä¸èƒ½å¤ªå¤šï¼Œæ‰€ä»¥éœ€è¦ä¸€å€‹ step size $t$ æˆ–ç¨±ç‚ºå­¸ç¿’ç‡

ä¹Ÿå°±æ˜¯èªª 

$$p \rightarrow t\nabla Cost(p)$$

æœƒåŸ·è¡Œç›´åˆ°è¶…å‡ºè¿­ä»£æ¬¡æ•¸ï¼Œæˆ–è€…é”åˆ°åœæ­¢æ¨™æº– (å’Œé æœŸç­”æ¡ˆè¶³å¤ é è¿‘)

### å›é¡§

æˆ‘å€‘çš„ $Cost$ å®šç¾©ç‚º 

$$Cost = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{2} \lVert y(x^{\{i\}}) - a^{[L]}(x^{\{i\}}) \rVert_2^2$$

æ¯å€‹åˆ†é‡ä¸Šçš„ $Cost$ ç‚º

$$C_{x^{\{i\}}} =  \frac{1}{2} \lVert y(x^{\{i\}}) - a^{[L]}(x^{\{i\}}) \rVert_2^2$$

æ‰€ä»¥å– gradient å¾Œç‚º

$$\nabla Cost(p) = \frac{1}{N} \sum_{i=1}^N \nabla C_{x^{\{i\}}}(p)$$

ä½†æ˜¯æœƒç™¼ç¾ï¼Œå¦‚æœæœ‰å¤§é‡åƒæ•¸ï¼Œæˆ–è€…å¤§é‡é»ï¼Œé€™æ¨£æœƒå°è‡´åšä¸€æ¬¡ gradient è¨ˆç®—æˆæœ¬éå¸¸æ˜‚è²´ã€‚

### stochastic gradient descent (SGD)

ä¸€ç¨®å¾ˆä¾¿å®œçš„æ›¿ä»£æ–¹æ¡ˆæ˜¯æ¯æ¬¡éƒ½éš¨æ©Ÿé¸æ“‡ä¸€å€‹é»çš„æ¢¯åº¦ï¼Œæ›¿æ›æ‰æ‰€æœ‰é»ä¸Šçš„å¹³å‡å€¼ï¼Œé€™ä¹Ÿæ˜¯éš¨æ©Ÿæ¢¯åº¦çš„æœ€ç°¡å–®å½¢å¼ã€‚æ–¹æ³•å¦‚ä¸‹

1. å¾$\{1,2,...N\}$  é¸æ“‡ä¸€å€‹"å‡å‹»éš¨æ©Ÿ"çš„å€¼ $i$
2. æ›´æ–° $$p \rightarrow t\Delta C_{x^{\{i\}}}(p)$$

æˆ–è€…æˆ‘å€‘ä¹Ÿå¯ä»¥éš¨æ©Ÿæ™‚ä¸å–å¾Œæ”¾å›ï¼Œä¹Ÿå°±æ˜¯éš¨æ©Ÿä¸€è¼ªï¼Œé€™ä¹Ÿç¨±ç‚ºå®Œæˆä¸€å€‹ epochã€‚

1. å¾$\{1,2,...N\}$ æ‰“äº‚æˆæ–°çš„æ’åº $\{k_1, k_2, k_3, ..., k_N\}$
2. $i$ å¾ $1$ åˆ° $N$ æ›´æ–° $$p \rightarrow t\Delta C_{x^{\{k_i\}}}(p)$$

å•é¡Œ: å¦‚æœå­¸ç¿’ç‡å¤ªå¤§ï¼Œå®¹æ˜“é€ æˆåƒæ•¸æ›´æ–°å‘ˆç¾é‹¸é½’ç‹€çš„æ›´æ–°ï¼Œé€™æ˜¯å¾ˆæ²’æœ‰æ•ˆç‡çš„è·¯å¾‘ã€‚

### mini-batch stochastic gradient descent

æŠŠè¨“ç·´é›†åˆ†å‰²æˆæ¯”è¼ƒå°çš„ï¼Œ ä¸€å †å°å°çš„è³‡æ–™é›†ï¼Œæˆ‘å€‘ç¨±ä»–å€‘ç¨±ç‚º "mini-batches" ã€‚ç•¶ä½ è¨“ç·´éå¸¸å¤šçš„è³‡æ–™ç›´åˆ°æ”¶æ–‚ç‚ºæ­¢çš„éç¨‹ï¼Œå°æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™è·‘å¾—æ¯”æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™é‚„å¿«å¾—å¤šã€‚

æ‰€ä»¥å‡è¨­ $m < N$ï¼Œä¹Ÿå°±æ˜¯ä¸€æ¬¡ä¸å– N ï¼Œè€Œæ˜¯ä¸€å€‹å°æ‰¹æ¬¡ m ã€‚


1. å¾ $\{k_1, k_2, k_3, ..., k_m\}$ å‡å‹»éš¨æ©Ÿå¾ $1 \sim N$ å–
2. æ›´æ–° $$p \rightarrow t \frac{1}{m} \sum_{i=1}^m\Delta C_{x^{\{k_i\}}}(p)$$

### Levenberg-Marquardt algorithm (LM)

èƒ½æä¾›æ•¸éç·šæ€§æœ€å°åŒ–ï¼ˆå±€éƒ¨æœ€å°ï¼‰çš„æ•¸å€¼è§£ã€‚æ­¤æ¼”ç®—æ³•èƒ½è—‰ç”±åŸ·è¡Œæ™‚ä¿®æ”¹åƒæ•¸é”åˆ°çµåˆé«˜æ–¯-ç‰›é “ç®—æ³•ä»¥åŠæ¢¯åº¦ä¸‹é™æ³•çš„å„ªé»ï¼Œä¸¦å°å…©è€…ä¹‹ä¸è¶³ä½œæ”¹å–„ï¼Œæ¯”å¦‚é«˜æ–¯-ç‰›é “ç®—æ³•ä¹‹åçŸ©é™£ä¸å­˜åœ¨æˆ–æ˜¯åˆå§‹å€¼é›¢å±€éƒ¨æ¥µå°å€¼å¤ªé 

$$p^{k+1} = p^k + (J^TJ + \mu I)^{-1}[J^T(y - F(p^k))]$$

- J ç‚º Jacobian 
- å¦‚æœ $\mu$ å¾ˆå°ï¼Œå…¶å¯¦å°±æ˜¯é«˜æ–¯ç‰›é “çš„æ–¹æ³•
- åä¹‹ï¼Œå°±æ˜¯æœ€é€Ÿä¸‹é™

å¯ä»¥æ³¨æ„åˆ°å› ç‚º $J$ ç‚º $m \times n$ å…¶ä¸­ $m$ ç‚ºè³‡æ–™é»ï¼Œ $n$ ç‚ºæ‰€æœ‰ parameter çš„æ•¸é‡ï¼Œ$J^TJ \in \mathbb{R}^{n \times n}$ï¼Œé€šå¸¸æƒ…æ³ä¸‹ $m$ >> $n$ï¼Œä½†å¦‚æœé‡åˆ° $n$ >> $m$ è©²å¦‚ä½•è§£æ±ºå‘¢ï¼Ÿ

### Woodbury matrix identity


åœ¨æ•¸å­¸ä¸­ï¼ˆç‰¹åˆ¥æ˜¯ç·šæ€§ä»£æ•¸é ˜åŸŸï¼‰ï¼ŒWoodbury çŸ©é™£æ†ç­‰å¼ä»¥ Max A. Woodbury å‘½åï¼Œè©²å…¬å¼è¡¨æ˜ï¼Œå°æŸå€‹çŸ©é™£é€²è¡Œç§©ç‚º ğ‘˜ çš„ä¿®æ­£ï¼Œå…¶é€†çŸ©é™£å¯ä»¥é€šéå°åŸå§‹çŸ©é™£çš„é€†çŸ©é™£é€²è¡Œç§©ç‚º ğ‘˜ çš„ä¿®æ­£ä¾†è¨ˆç®—ã€‚é€™å€‹å…¬å¼çš„å…¶ä»–åç¨±åŒ…æ‹¬çŸ©é™£åæ¼”å¼•ç†ï¼ˆMatrix Inversion Lemmaï¼‰ã€Shermanâ€“Morrisonâ€“Woodbury å…¬å¼æˆ–åƒ…ç¨±ç‚º Woodbury å…¬å¼ã€‚

Woodbury çŸ©é™£æ†ç­‰å¼çš„å…·é«”å½¢å¼ç‚º:

$$(A + UCV)^{-1} = A^{-1}U(C^{-1} + VA^{-1}U)^{-1} V A^{-1}$$

æ‰€ä»¥å¥—ç”¨åœ¨ L-M ä¸Šï¼Œå‡è¨­

$A = \lambda I$, $U = J^T$, $B = I$, $V = J$ ï¼Œæ‰€ä»¥å¾—åˆ°

$$( \lambda I + J^T J)^{-1} = \lambda^{-1}I - \lambda^{-1}J^T(\lambda I + JJ^T)^{-1} J \lambda ^{-1}$$


æœƒç™¼ç¾æˆ‘å€‘æˆåŠŸæŠŠ $J^TJ$ æ”¹ç‚º $JJ^T$
## Back propagation (BP)

ä¸­æ–‡ç¨±åå‘å‚³æ’­ï¼Œæ˜¯å°å¤šå±¤é¡ç¥ç¶“ç¶²è·¯é€²è¡Œæ¢¯åº¦ä¸‹é™çš„æ¼”ç®—æ³•ï¼Œä¹Ÿå°±æ˜¯ç”¨éˆå¼æ³•å‰‡ (chain rule) ä»¥ç¶²è·¯æ¯å±¤çš„æ¬Šé‡ç‚ºè®Šæ•¸è¨ˆç®—æå¤±å‡½å¼çš„æ¢¯åº¦ï¼Œä»¥æ›´æ–°æ¬Šé‡ä¾†æœ€å°åŒ–æå¤±å‡½å¼ã€‚

ç¾åœ¨å…ˆå°‡é—œæ³¨åœ¨å–®ç¨çš„åå°æ•¸ï¼Œå‰›å‰›çš„ $C_{x^{\{i\}}}$ å¯ä»¥å…ˆæ¨å» $x^{\{i\}}$ï¼Œç°¡åŒ–ä¸€ä¸‹å¾—åˆ°

$$C = \frac{1}{2} \Vert y - a^{[L]} \Vert_2^2$$

å›é¡§ä¸€ä¸‹ç¥ç¶“ç¶²è·¯çš„ä¸€èˆ¬å½¢å¼

$$ \begin{cases}
a^{[1]} = x\\
a^{[l]} = \sigma(W^{[l]}a^{[l-1]} +b^{[l]}), \space for \space l = 2,3,...,L\\
\end{cases}$$ 

ç‚ºäº†æ–¹ä¾¿èµ·è¦‹ï¼Œæˆ‘å€‘å®šç¾©ä¸€å€‹æ–°çš„è®Šæ•¸ $z$ å¦‚ä¸‹

$$z^{[l]} = W^{[l]}a^{[l-1]} +b^{[l]}$$

è€Œ $z_j^{[l]}$ æ„æ€ç‚ºè¼¸å…¥åœ¨ç¬¬ $l$ å±¤ç¥ç¶“å…ƒ $j$ çš„æ¬Šé‡è¼¸å…¥ã€‚

æ‰€ä»¥ä¸€èˆ¬å½¢å¼å¯ä»¥ç°¡åŒ–å¦‚ä¸‹

$$ \begin{cases}
a^{[1]} = x\\
a^{[l]} = \sigma(z^{[l]}), \space for \space l = 2,3,...,L\\
\end{cases}$$ 

ç¾åœ¨ï¼Œå®šç¾© $C$ åå°æ•¸ç‚º $\delta^{[l]}$

$$\delta^{[l]} = \frac{\partial C}{\partial z_j^{[l]}}, for \space 1  \leq j \leq n_l \space, \space 2 \leq l \leq L$$

é€™å€‹è¡¨é”å¼é€šå¸¸è¢«ç¨±ç‚ºåœ¨ç¬¬ $l$ å±¤ç¥ç¶“å…ƒ $j$ çš„èª¤å·® errorã€‚ å¯¦éš›ä¸Šè¢«ç¨±ç‚ºèª¤å·®é‚„æ˜¯æœ‰ä¸€é»æ¨¡ç³Šçš„ï¼Œå¯ä»¥çŸ¥é“æˆ‘å€‘çš„ç›®çš„ç‚ºæœ€å°åŒ– $Cost$ï¼Œæ‰€ä»¥ $\delta = 0$ æ˜¯ä¸€å€‹ç›®æ¨™ï¼Œé€™ä¹Ÿæ˜¯ç‚ºä»€éº¼ $\delta$ è¢«ç¨±ç‚ºèª¤å·®åŸå› ã€‚

å‡è¨­ $x, y \in \mathbb{R}^n$ å®šç¾©ç‚º $(x \odot y)_i = x_i y_i$ï¼Œä¹Ÿå°±æ˜¯èªªä»–çš„ Hadamard ä¹˜ç©å°±æ˜¯ç”±å°æ‡‰çš„åˆ†é‡å…©å…©ç›¸ä¹˜èµ·ä¾†ã€‚

![](img/NN/7.png)

![](img/NN/8.png)

åˆ©ç”¨é€™ç¨®è¡¨ç¤ºæ³•ï¼Œå¯ä»¥æ¨å‡ºä»¥ä¸‹éŠå¼æ³•å‰‡çµæœ

### Lemma 1

$$\delta ^{[L]} = \sigma ' (z^{[L]}) \odot (a^{[L]} - y) \tag{1.1}$$
$$\delta ^{[l]} = \sigma ' (z^{[l]}) \odot (W^{[l+1]})^T \delta^{[l+1]}, \space for \space 2 \leq l \leq L-1 \tag{1.2}$$
$$\frac{\partial C}{\partial b_j^{[l]}} = \delta_j^{[l]}, \space for \space 2 \leq l \leq L \tag{1.3}$$
$$ \frac{\partial C}{\partial w_{jk}^{[l]}} = \delta_j^{[l]} a_k^{[l-1]}, \space for \space 2 \leq l \leq L \tag{1.4}$$

è­‰æ˜ç•¥ï¼Œæœ‰èˆˆè¶£å¯ä»¥ç›´æ¥åƒè€ƒè«–æ–‡è£¡é¢çš„è­‰æ˜ã€‚ 

é€™äº›é—œä¿‚å¼æœ‰å¾ˆå¤šå€¼å¾—é—œæ³¨çš„åœ°æ–¹ã€‚æ³¨æ„åˆ°ï¼Œå‰é¢æˆ‘å€‘æœ‰

$$ \begin{cases}
a^{[1]} = x\\
a^{[l]} = \sigma(z^{[l]}), \space for \space l = 2,3,...,L\\
z^{[l]} = W^{[l]}a^{[l-1]} +b^{[l]}\\
\end{cases}$$ 

ä¹Ÿå°±æ˜¯å¯ä»¥ $a^{[L]}$ å¾ $a^{[1]}, z^{[2]}, a^{[2]}, z^{[3]}, ..., a^{[L]}$ å‰å‘æ¨å¾—ã€‚

å†å¾ $Lemma \space 1.1$ å¯ä»¥ç«‹å³æ¨å¾— $\delta^{[L]}$ ç¹¼çºŒé€é $Lemma \space 1.2$ å¯ä»¥ "åå‘" æ¨å¾— $\delta^{[l-1]}, \delta^{[l-2]}, ... ,\delta^{[2]}$

å†é€é $Lemma \space 1.3$, $Lemma \space 1.4$ å¯ä»¥å¾—åˆ°åå°æ•¸

:::info
gradient ä»¥é€™ç¨®æ–¹å¼æ¨å°å‡ºä¾†çš„ï¼Œåˆç¨±ç‚º back propagation åå‘æ¨å°ã€‚ lemma 1.3 1.4 ä¹Ÿç¨±ç‚º back propagation formula 
:::

<!doctype html>
<html lang="en-US" data-theme="dark">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.19" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.71" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://hippotumux.github.io/blog/blog/Posts/NLP/NLP3.html"><meta property="og:site_name" content="Hippotumux's Blog"><meta property="og:title" content="NLP word embedding and RNN"><meta property="og:description" content="高宏宇 - 教授 N-grams N-gram 是一個包含 n 個單詞的序列。通過不同數量的單詞組合，N-gram 模型可以捕捉語言中的上下文關係。 1-gram（unigram）：單詞序列中的每個單詞單獨作為一個項目，例如： "please" "turn" "your" "homework" 2-gram（bigram）：兩個連續的單詞組合，例如： ..."><meta property="og:type" content="article"><meta property="og:image" content="https://hackmd.io/_uploads/r1RzDmepA.png"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-02-24T03:41:58.000Z"><meta property="article:tag" content="note"><meta property="article:tag" content="theorem"><meta property="article:published_time" content="2024-10-04T00:00:00.000Z"><meta property="article:modified_time" content="2025-02-24T03:41:58.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"NLP word embedding and RNN","image":["https://hackmd.io/_uploads/r1RzDmepA.png","https://hackmd.io/_uploads/H1i-umla0.png","https://hackmd.io/_uploads/HysBuQgaR.png","https://hackmd.io/_uploads/HybAKmgpR.png","https://hackmd.io/_uploads/Bk5ec7lTR.png","https://hackmd.io/_uploads/rkxHcmxTR.png","https://hackmd.io/_uploads/SkJrnXx6A.png","https://hackmd.io/_uploads/By0AyVlTR.png","https://hackmd.io/_uploads/r1bvXVeaC.png","https://hackmd.io/_uploads/B14tL4e6A.png"],"datePublished":"2024-10-04T00:00:00.000Z","dateModified":"2025-02-24T03:41:58.000Z","author":[{"@type":"Person","name":"Hippotumux","url":"https://hippotumux.github.io/blog/"}]}</script><link rel="icon" href="Mylogo.jpg"><title>NLP word embedding and RNN | Hippotumux's Blog</title><meta name="description" content="高宏宇 - 教授 N-grams N-gram 是一個包含 n 個單詞的序列。通過不同數量的單詞組合，N-gram 模型可以捕捉語言中的上下文關係。 1-gram（unigram）：單詞序列中的每個單詞單獨作為一個項目，例如： "please" "turn" "your" "homework" 2-gram（bigram）：兩個連續的單詞組合，例如： ...">
    <link rel="stylesheet" href="/blog/assets/css/styles.2ad513e2.css">
    <link rel="preload" href="/blog/assets/js/runtime~app.56e075d7.js" as="script"><link rel="preload" href="/blog/assets/css/styles.2ad513e2.css" as="style"><link rel="preload" href="/blog/assets/js/408.dd242f5f.js" as="script"><link rel="preload" href="/blog/assets/js/app.1a083678.js" as="script">
    <link rel="prefetch" href="/blog/assets/js/Posts_RL_markov_decision.html.5a50040d.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_NN.html.bd43b13d.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_ML2.html.638b97ba.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_ML1.html.601229bf.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP4.html.2feb6cf3.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP2.html.8005f102.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP3.html.5938d056.js" as="script"><link rel="prefetch" href="/blog/assets/js/8300.2a39983f.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_RL_QL_snake.html.9cbe1804.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_markdown.html.b30e427e.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP1.html.4773eae2.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_ML0.html.9a117ce9.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_RAG.html.2c1928b0.js" as="script"><link rel="prefetch" href="/blog/assets/js/intro.html.fa0bbf18.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_page.html.70d09695.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_layout.html.eb3f5b24.js" as="script"><link rel="prefetch" href="/blog/assets/js/cool.html.dd8ee282.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_disable.html.f0336708.js" as="script"><link rel="prefetch" href="/blog/assets/js/index.html.6e782bed.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_index.html.b26164d5.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_index.html.63d19d0a.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_encrypt.html.3f85a1ef.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_使用指南_index.html.2f018673.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_使用指南_index.html.09488c89.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_页面配置_index.html.87bbd16a.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_指南_index.html.8b205227.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_probability_index.html.9376bbbf.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_diary_index.html.c81e785c.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_posts_index.html.7449135b.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_加密_index.html.8f34de35.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_禁用_index.html.cd7492a1.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_布局_index.html.5e2962b5.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_markdown_index.html.857b4a85.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_nlp_index.html.1bd6bf70.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_theorem_index.html.59252ad7.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_ml_index.html.7e5a8f93.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_python_index.html.2fc5e11f.js" as="script"><link rel="prefetch" href="/blog/assets/js/404.html.ae79f368.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_note_index.html.70218866.js" as="script"><link rel="prefetch" href="/blog/assets/css/3817.styles.d4f8889c.css" as="style"><link rel="prefetch" href="/blog/assets/js/tag_imp_index.html.6b2baddd.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_pdf_index.html.0cc415f9.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_index.html.d8997b45.js" as="script"><link rel="prefetch" href="/blog/assets/js/timeline_index.html.9d6bff97.js" as="script"><link rel="prefetch" href="/blog/assets/js/article_index.html.037bcb43.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_index.html.685e29b4.js" as="script"><link rel="prefetch" href="/blog/assets/js/star_index.html.393200c5.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_index.html.43126e4a.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_index.html.df9d0fe8.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_RL_index.html.50262dca.js" as="script"><link rel="prefetch" href="/blog/assets/css/2087.styles.155122d8.css" as="style">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/blog/" aria-label="Take me home"><img class="vp-nav-logo" src="/blog/assets/images/Mylogo.jpg" alt><!----><span class="vp-site-name hide-in-pad">Hippotumux&#39;s Blog</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/" aria-label="Home" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" height="1em" sizing="height"></iconify-icon><!--]-->Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/demo/" aria-label="Projects" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:book-2-outline" height="1em" sizing="height"></iconify-icon><!--]-->Projects<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link route-link-active auto-link" href="/blog/Posts/" aria-label="Posts" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:add-notes-outline" height="1em" sizing="height"></iconify-icon><!--]-->Posts<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/intro.html" aria-label="About Me" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:family-star" height="1em" sizing="height"></iconify-icon><!--]-->About Me<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/Hippotumux/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/" aria-label="Home" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->Home<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><iconify-icon class="vp-icon" icon="material-symbols:book-2-outline" width="1em" height="1em" sizing="both"></iconify-icon><a class="route-link auto-link vp-sidebar-title no-external-link-icon" href="/blog/demo/" aria-label="Projects" iconsizing="both"><!---->Projects<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/markdown.html" aria-label="Markdown 展示" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-brands:markdown" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->Markdown 展示<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/layout.html" aria-label="你好" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:object-group" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->你好<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/page.html" aria-label="页面配置" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->页面配置<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/disable.html" aria-label="布局与功能禁用" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:gears" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->布局与功能禁用<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/encrypt.html" aria-label="密码加密的文章" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:lock" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->密码加密的文章<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable active"><iconify-icon class="vp-icon" icon="material-symbols:add-notes-outline" width="1em" height="1em" sizing="both"></iconify-icon><a class="route-link route-link-active auto-link vp-sidebar-title no-external-link-icon" href="/blog/Posts/" aria-label="Posts" iconsizing="both"><!---->Posts<!----></a><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">ML</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">NLP</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/NLP1.html" aria-label="NLP 簡介" iconsizing="both"><!---->NLP 簡介<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/NLP2.html" aria-label="NLP 簡介II" iconsizing="both"><!---->NLP 簡介II<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/blog/Posts/NLP/NLP3.html" aria-label="NLP word embedding and RNN" iconsizing="both"><!---->NLP word embedding and RNN<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/NLP4.html" aria-label="NLP Sequence-to-sequnce Model" iconsizing="both"><!---->NLP Sequence-to-sequnce Model<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/RAG.html" aria-label="RAG 介紹" iconsizing="both"><!---->RAG 介紹<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">RL</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/intro.html" aria-label="About Me" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:family-star" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->About Me<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->NLP word embedding and RNN</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://hippotumux.github.io/blog/" target="_blank" rel="noopener noreferrer">Hippotumux</a></span><span property="author" content="Hippotumux"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">October 4, 2024</span><meta property="datePublished" content="2024-10-04T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 12 min</span><meta property="timeRequired" content="PT12M"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color2 clickable" role="navigation">NLP</span><!--]--><meta property="articleSection" content="NLP"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color4 clickable" role="navigation">note</span><span class="page-tag-item color0 clickable" role="navigation">theorem</span><!--]--><meta property="keywords" content="note,theorem"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><!--[--><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#n-grams">N-grams</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#n-gram-語言模型">N-gram 語言模型</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#bi-gram-模型示例">Bi-gram 模型示例</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#模型比較">模型比較</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#困惑度-perplexity-ppl">困惑度（Perplexity, PPL）</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#意義">意義</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#word-embedding">Word Embedding</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#嵌入的特性">嵌入的特性</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#word2vec">Word2Vec</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#word2vec-流程">Word2Vec 流程</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#上下文嵌入-contextualized-word-embeddings">上下文嵌入（Contextualized Word Embeddings）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#神經網絡架構中的嵌入層和語言模型層">神經網絡架構中的嵌入層和語言模型層</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#神經語言模型">神經語言模型</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#深度學習基礎">深度學習基礎</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#訓練神經網絡">訓練神經網絡</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#激活函數">激活函數</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#ffn">FFN</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#前饋神經網絡-ffn">前饋神經網絡（FFN）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#前饋神經網絡-ffn-的缺點">前饋神經網絡（FFN）的缺點</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#rnn">RNN</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#遞歸神經網絡-rnn">遞歸神經網絡（RNN）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#rnn-的特性">RNN 的特性</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#rnn-應用">RNN 應用</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#rnn-應用於命名實體識別-ner">RNN 應用於命名實體識別（NER）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#rnn-應用於序列分類">RNN 應用於序列分類</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#堆疊式-rnns">堆疊式 RNNs</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#雙向-rnns">雙向 RNNs</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#用於序列分類的雙向-rnns">用於序列分類的雙向 RNNs</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#總結">總結</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#嵌入方法">嵌入方法：</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#神經語言模型-neural-lms">神經語言模型（Neural LMs）：</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--]--><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="nlp-word-embedding-and-rnn" tabindex="-1"><a class="header-anchor" href="#nlp-word-embedding-and-rnn"><span>NLP word embedding and RNN</span></a></h1><p>高宏宇 - 教授</p><h2 id="n-grams" tabindex="-1"><a class="header-anchor" href="#n-grams"><span>N-grams</span></a></h2><p><strong>N-gram</strong> 是一個包含 n 個單詞的序列。通過不同數量的單詞組合，N-gram 模型可以捕捉語言中的上下文關係。</p><ul><li><strong>1-gram（unigram）</strong>：單詞序列中的每個單詞單獨作為一個項目，例如： <ul><li>&quot;please&quot;</li><li>&quot;turn&quot;</li><li>&quot;your&quot;</li><li>&quot;homework&quot;</li></ul></li><li><strong>2-gram（bigram）</strong>：兩個連續的單詞組合，例如： <ul><li>&quot;please turn&quot;</li><li>&quot;turn your&quot;</li><li>&quot;your homework&quot;</li></ul></li><li><strong>3-gram（trigram）</strong>：三個連續的單詞組合，例如： <ul><li>&quot;please turn your&quot;</li><li>&quot;turn your homework&quot;</li></ul></li><li><strong>N-gram</strong> 模型可擴展至更高的數值，例如 4-gram、n-gram 等。</li></ul><!-- more --><h3 id="n-gram-語言模型" tabindex="-1"><a class="header-anchor" href="#n-gram-語言模型"><span>N-gram 語言模型</span></a></h3><p>我們可以使用一種簡單的統計方法來構建 N-gram 語言模型，該模型依賴於計算特定 n-gram 在語料庫中出現的次數。該方法假設詞語的出現僅取決於前 n-1 個詞，而不考慮更長距離的語境。</p><h3 id="bi-gram-模型示例" tabindex="-1"><a class="header-anchor" href="#bi-gram-模型示例"><span>Bi-gram 模型示例</span></a></h3><p>假設有以下句子：</p><ul><li>&quot;I want to eat lunch.&quot;</li><li>&quot;I want to eat Chinese food.&quot;</li><li>&quot;I don&#39;t want to spend time cooking.&quot;</li></ul><p>在 Bi-gram 模型中，我們需要統計每個二元詞組出現的次數。例如：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mtext>I want</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">C(\text{I want}) = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord text"><span class="mord">I want</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mtext>want to</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">C(\text{want to}) = 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord text"><span class="mord">want to</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mtext>spend time</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">C(\text{spend time}) = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord text"><span class="mord">spend time</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></li></ul><p>這樣的模型可以通過統計不同 n-gram 的出現次數來預測接下來的單詞，從而達到基本的語言生成和建模的效果。</p><h3 id="模型比較" tabindex="-1"><a class="header-anchor" href="#模型比較"><span>模型比較</span></a></h3><h4 id="_1-n-gram-語言模型" tabindex="-1"><a class="header-anchor" href="#_1-n-gram-語言模型"><span>1. <strong>N-Gram 語言模型</strong></span></a></h4><ul><li><strong>定義</strong>：一種純粹的統計模型，基於先前出現的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 個詞來預測接下來的詞，並計算詞序列的出現概率。</li><li><strong>特點</strong>：N-Gram 模型簡單易用，但它僅考慮有限的上下文，難以捕捉長距離依賴。</li></ul><h4 id="_2-神經網絡語言模型-neural-language-models" tabindex="-1"><a class="header-anchor" href="#_2-神經網絡語言模型-neural-language-models"><span>2. <strong>神經網絡語言模型（Neural Language Models）</strong></span></a></h4><ul><li><strong>定義</strong>：利用神經網絡來預測詞語序列的可能性，通過學習詞向量來捕捉語義和語法關係。</li><li><strong>特點</strong>：相比 N-Gram 模型，神經網絡語言模型能更好地處理長距離依賴，並且能學習到更豐富的語言結構。</li></ul><h2 id="困惑度-perplexity-ppl" tabindex="-1"><a class="header-anchor" href="#困惑度-perplexity-ppl"><span>困惑度（Perplexity, PPL）</span></a></h2><p><strong>困惑度（Perplexity, PPL）</strong> 是一種量化標準，用來評估語言模型的能力或效果。</p><ul><li><p><strong>定義</strong>：給定詞語序列和一個 N-gram 模型，困惑度是通過以下公式計算的：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>P</mi><mi>L</mi><mo>=</mo><msup><mn>2</mn><mrow><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mtext> </mtext><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">PPL = 2^{-\frac{1}{N} \sum_{i=1}^N \log_2 P(w_i \mid w_{i-1}, w_{i-2}, \dots)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">PP</span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0564em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0564em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mop mtight"><span class="mtight">l</span><span class="mtight">o</span><span class="mtight" style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span style="top:-2.2341em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2659em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mrel mtight">∣</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="minner mtight">…</span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></p></li><li><p><strong>解釋</strong>：困惑度值越低，表示語言模型的能力越好，能更準確地進行詞語預測。</p></li></ul><h3 id="意義" tabindex="-1"><a class="header-anchor" href="#意義"><span>意義</span></a></h3><ol><li><strong>不確定性的度量</strong>： <ul><li>困惑度量化了模型在進行預測時所面臨的不確定性或不可預測性。</li></ul></li><li><strong>平均分支因子</strong>： <ul><li>困惑度可以視為在每個序列步驟中「平均分支因子」的度量，表示在每一步中模型可能選擇的選項數量。</li></ul></li><li><strong>模型表現的量化</strong>： <ul><li>在語言建模中，困惑度反映了模型對語言規則和結構的理解程度。</li><li>困惑度越低，表明模型對語言的理解和預測能力越好，表明模型能有效捕捉語言模式。</li></ul></li><li><strong>壓縮效率的指標</strong>： <ul><li>困惑度也可以視為模型如何有效壓縮測試數據的度量。</li><li>困惑度越低，表示模型以更高的概率進行預測，減少了不確定性，從而有效地壓縮了信息。</li></ul></li></ol><h2 id="word-embedding" tabindex="-1"><a class="header-anchor" href="#word-embedding"><span>Word Embedding</span></a></h2><p><strong>Word Embedding</strong> 是一種將詞語表示為連續向量的技術，這些向量能夠捕捉詞語之間的語義和語法關係。</p><h4 id="_1-概念空間-concept-space" tabindex="-1"><a class="header-anchor" href="#_1-概念空間-concept-space"><span>1. <strong>概念空間（Concept Space）</strong></span></a></h4><ul><li><strong>定義</strong>：概念空間是指詞嵌入向量所在的向量空間，詞語以稠密向量的形式分布在這個空間中，能夠反映詞語之間的語義相似性。</li></ul><h4 id="_2-詞項向量-term-vector" tabindex="-1"><a class="header-anchor" href="#_2-詞項向量-term-vector"><span>2. <strong>詞項向量（Term Vector）</strong></span></a></h4><ul><li><strong>定義</strong>：詞項向量是指每個詞語對應的一個稠密向量，這些向量用來表示詞語在概念空間中的位置。</li></ul><h4 id="_3-稠密向量-dense-vectors" tabindex="-1"><a class="header-anchor" href="#_3-稠密向量-dense-vectors"><span>3. <strong>稠密向量（Dense Vectors）</strong></span></a></h4><ul><li><strong>定義</strong>：稠密向量嵌入將詞語表示在連續的向量空間中，詞語之間的相似性可以根據向量之間的距離來衡量。</li><li><strong>特性</strong>：語義相似的詞會在空間中靠得更近。</li></ul><h4 id="_4-word2vec" tabindex="-1"><a class="header-anchor" href="#_4-word2vec"><span>4. <strong>Word2Vec</strong></span></a></h4><ul><li><strong>Word2Vec</strong> 是一種常用的詞嵌入方法，它基於上下文來學習詞語的向量表示，能夠捕捉到詞語的語義信息。</li></ul><h4 id="_5-上下文嵌入-contextualized-embeddings" tabindex="-1"><a class="header-anchor" href="#_5-上下文嵌入-contextualized-embeddings"><span>5. <strong>上下文嵌入（Contextualized Embeddings）</strong></span></a></h4><ul><li><strong>定義</strong>：上下文嵌入會根據詞語所處的語境來動態調整詞嵌入向量，讓相同的詞在不同的語境中可以有不同的表示。</li></ul><h3 id="嵌入的特性" tabindex="-1"><a class="header-anchor" href="#嵌入的特性"><span>嵌入的特性</span></a></h3><ol><li><strong>類比關係/關聯相似性</strong><ul><li><p>詞嵌入可以捕捉詞語之間的類比關係。例如：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Washington</mtext><mo>−</mo><mtext>U.S.</mtext><mo>=</mo><mtext>London</mtext><mo>−</mo><mtext>U.K.</mtext></mrow><annotation encoding="application/x-tex">\text{Washington} - \text{U.S.} = \text{London} - \text{U.K.} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Washington</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">U.S.</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">London</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">U.K.</span></span></span></span></span></span></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Washington</mtext><mo>−</mo><mtext>U.S.</mtext><mo>+</mo><mtext>U.K.</mtext><mo>=</mo><mtext>London</mtext></mrow><annotation encoding="application/x-tex">\text{Washington} - \text{U.S.} + \text{U.K.} = \text{London} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Washington</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">U.S.</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">U.K.</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">London</span></span></span></span></span></span></p></li></ul></li></ol><h2 id="word2vec" tabindex="-1"><a class="header-anchor" href="#word2vec"><span>Word2Vec</span></a></h2><p><strong>Word2Vec</strong> 是一種詞嵌入技術，通過將詞語表示為向量，捕捉詞語的語義關係。Word2Vec 有兩種主要的模型架構：<strong>Skip-gram</strong> 和 <strong>CBOW</strong>。</p><h4 id="_1-skip-gram" tabindex="-1"><a class="header-anchor" href="#_1-skip-gram"><span>1. <strong>Skip-gram</strong></span></a></h4><ul><li><strong>定義</strong>：使用目標詞來預測其上下文詞語。</li><li><strong>原理</strong>：Skip-gram 模型的目標是給定一個中心詞，預測這個詞周圍的上下文詞語。這使得模型能夠學習到詞與其上下文的語義聯繫，特別適合於處理稀疏數據。</li></ul><h4 id="_2-cbow-continuous-bag-of-words" tabindex="-1"><a class="header-anchor" href="#_2-cbow-continuous-bag-of-words"><span>2. <strong>CBOW（Continuous Bag of Words）</strong></span></a></h4><ul><li><strong>定義</strong>：使用上下文詞來預測目標詞。</li><li><strong>原理</strong>：CBOW 模型的目標是根據周圍的上下文詞語來預測中心詞。這種模型利用上下文信息來預測詞語，對於捕捉局部語境非常有效。</li></ul><figure><img src="/blog/assets/img/1.6c004dd7.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="word2vec-流程" tabindex="-1"><a class="header-anchor" href="#word2vec-流程"><span>Word2Vec 流程</span></a></h3><h4 id="_1-將目標詞與其鄰近的上下文詞作為正樣本" tabindex="-1"><a class="header-anchor" href="#_1-將目標詞與其鄰近的上下文詞作為正樣本"><span>1. <strong>將目標詞與其鄰近的上下文詞作為正樣本</strong></span></a></h4><h4 id="_2-隨機從詞彙表中抽取其他詞作為負樣本" tabindex="-1"><a class="header-anchor" href="#_2-隨機從詞彙表中抽取其他詞作為負樣本"><span>2. <strong>隨機從詞彙表中抽取其他詞作為負樣本</strong></span></a></h4><h4 id="_3-使用邏輯回歸訓練分類器以區分正負樣本" tabindex="-1"><a class="header-anchor" href="#_3-使用邏輯回歸訓練分類器以區分正負樣本"><span>3. <strong>使用邏輯回歸訓練分類器以區分正負樣本</strong></span></a></h4><h4 id="_4-使用學到的權重作為嵌入向量" tabindex="-1"><a class="header-anchor" href="#_4-使用學到的權重作為嵌入向量"><span>4. <strong>使用學到的權重作為嵌入向量</strong></span></a></h4><figure><img src="/blog/assets/img/2.21cbfb29.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="上下文嵌入-contextualized-word-embeddings" tabindex="-1"><a class="header-anchor" href="#上下文嵌入-contextualized-word-embeddings"><span>上下文嵌入（Contextualized Word Embeddings）</span></a></h3><p><strong>上下文嵌入</strong> 是一種將詞嵌入與模型緊密結合的技術。它能根據具體任務的上下文動態調整詞語的表示，使模型能夠更好地理解詞語在不同上下文中的語義。</p><h4 id="_1-嵌入設計為模型的一部分" tabindex="-1"><a class="header-anchor" href="#_1-嵌入設計為模型的一部分"><span>1. <strong>嵌入設計為模型的一部分</strong></span></a></h4><h4 id="_2-根據下游任務調整" tabindex="-1"><a class="header-anchor" href="#_2-根據下游任務調整"><span>2. <strong>根據下游任務調整</strong></span></a></h4><h3 id="神經網絡架構中的嵌入層和語言模型層" tabindex="-1"><a class="header-anchor" href="#神經網絡架構中的嵌入層和語言模型層"><span>神經網絡架構中的嵌入層和語言模型層</span></a></h3><ul><li>在神經網絡架構中，詞嵌入層通常作為輸入層，將詞語轉換為向量。</li><li>語言模型層（LM Layers）則可以進一步處理這些嵌入，捕捉更多的語法和語義信息。</li></ul><figure><img src="/blog/assets/img/3.c0d2ce3e.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="神經語言模型" tabindex="-1"><a class="header-anchor" href="#神經語言模型"><span>神經語言模型</span></a></h2><p>需要一個模型結構來處理嵌入中的隱藏特徵。神經語言模型利用神經網絡來學習和表示複雜的語言模式。</p><ul><li><strong>FFN（前饋神經網絡，Feedforward Network）</strong></li><li><strong>RNN（循環神經網絡，Recurrent Neural Networks）</strong></li></ul><h3 id="深度學習基礎" tabindex="-1"><a class="header-anchor" href="#深度學習基礎"><span>深度學習基礎</span></a></h3><ol><li><strong>模型（Model）</strong><ul><li>模型指的是用來表示輸入數據與輸出預測之間關係的架構或結構。</li></ul></li><li><strong>優化器（Optimizer）</strong><ul><li>優化器是一種算法，用於在訓練過程中調整模型的參數，以最小化預測值與實際輸出值之間的誤差。</li></ul></li><li><strong>損失函數（Loss Function）</strong><ul><li>損失函數（目標函數）用來衡量模型的預測輸出與真實目標輸出之間的差異。</li></ul></li></ol><h3 id="訓練神經網絡" tabindex="-1"><a class="header-anchor" href="#訓練神經網絡"><span>訓練神經網絡</span></a></h3><p>訓練過程通常包括以下步驟：</p><ol><li><strong>數據準備</strong>：準備訓練和測試數據集。</li><li><strong>模型構建</strong>：使用深度學習框架（TensorFlow, PyTorch 等）構建模型。</li><li><strong>損失函數定義</strong>：選擇適當的損失函數。（交叉熵，Logloss 等）</li><li><strong>優化器選擇</strong>：選擇合適的優化算法。（Adam, SGD 等）</li><li><strong>模型訓練</strong>：使用訓練數據集訓練模型。</li><li><strong>模型評估</strong>：使用測試數據集評估訓練好的模型。（F1, LCS 等）</li></ol><h3 id="激活函數" tabindex="-1"><a class="header-anchor" href="#激活函數"><span>激活函數</span></a></h3><ul><li>使用激活函數的核心思想是為神經網絡引入非線性。</li><li>神經網絡模型旨在避免最終的處理階段僅僅是輸入的線性變換，從而使模型能夠在複雜問題上有良好的表現。</li><li>常用的激活函數包括：Sigmoid, ReLU, tanh, GeLU。</li></ul><h2 id="ffn" tabindex="-1"><a class="header-anchor" href="#ffn"><span>FFN</span></a></h2><h3 id="前饋神經網絡-ffn" tabindex="-1"><a class="header-anchor" href="#前饋神經網絡-ffn"><span>前饋神經網絡（FFN）</span></a></h3><ul><li>前饋神經網絡（FFN）是一種多層的前饋網絡，網絡中的單元之間的連接沒有循環。<br><img src="/blog/assets/img/4.90c09f88.png" alt="" loading="lazy"></li></ul><h3 id="前饋神經網絡-ffn-的缺點" tabindex="-1"><a class="header-anchor" href="#前饋神經網絡-ffn-的缺點"><span>前饋神經網絡（FFN）的缺點</span></a></h3><ul><li><strong>缺乏序列建模</strong>： <ul><li>在自然語言處理（NLP）任務中，理解詞語的順序及其依賴關係對準確的預測至關重要。</li></ul></li><li><strong>固定輸入大小</strong>： <ul><li>FFN 需要固定大小的輸入，這在 NLP 任務中會產生問題，因為輸入序列的長度通常是變化的。</li></ul></li><li><strong>有限的上下文信息</strong>： <ul><li>許多 NLP 任務需要捕捉長距依賴性和理解文本的廣泛上下文，而這些更適合能夠有效建模序列數據的模型。</li></ul></li></ul><h2 id="rnn" tabindex="-1"><a class="header-anchor" href="#rnn"><span>RNN</span></a></h2><h3 id="遞歸神經網絡-rnn" tabindex="-1"><a class="header-anchor" href="#遞歸神經網絡-rnn"><span>遞歸神經網絡（RNN）</span></a></h3><ul><li>在時間 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 的方程式如下：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>V</mi><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y_t = g(Vh_t) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>U</mi><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi>W</mi><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = f(Ux_t + Wh_{t-1}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p></li><li>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span> 是激活函數。</li></ul><figure><img src="data:image/png;base64,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" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="rnn-的特性" tabindex="-1"><a class="header-anchor" href="#rnn-的特性"><span>RNN 的特性</span></a></h3><ul><li><strong>序列處理</strong>： <ul><li>RNN 能處理序列數據，允許它們模擬數據中的時間依賴性。</li></ul></li><li><strong>遞歸連接</strong>： <ul><li>RNN 維持內部記憶，有助於捕捉長期依賴。</li></ul></li><li><strong>參數共享</strong>： <ul><li>RNN 在不同時間共享參數，增強了學習序列數據的效率。</li></ul></li><li><strong>梯度消失問題</strong>： <ul><li>傳統的 RNN 可能會遇到梯度消失的問題，這阻礙了長期依賴的學習。</li></ul></li></ul><h2 id="rnn-應用" tabindex="-1"><a class="header-anchor" href="#rnn-應用"><span>RNN 應用</span></a></h2><h3 id="rnn-應用於命名實體識別-ner" tabindex="-1"><a class="header-anchor" href="#rnn-應用於命名實體識別-ner"><span>RNN 應用於命名實體識別（NER）</span></a></h3><ul><li>在標記分類任務中，每個輸出應該映射到一個 one-hot 向量。</li><li>在 RNN 模型中增加了一個前饋網絡。</li></ul><figure><img src="/blog/assets/img/6.e6517413.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="rnn-應用於序列分類" tabindex="-1"><a class="header-anchor" href="#rnn-應用於序列分類"><span>RNN 應用於序列分類</span></a></h3><ul><li>RNN 用於對整個序列進行分類，而不是序列中的單個標記。</li><li>取文本最後一個標記的隱藏層。</li></ul><figure><img src="data:image/png;base64,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" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="堆疊式-rnns" tabindex="-1"><a class="header-anchor" href="#堆疊式-rnns"><span>堆疊式 RNNs</span></a></h3><ul><li>堆疊式 RNNs 由多個網絡組成，其中一層的輸出作為後續層的輸入。</li></ul><figure><img src="/blog/assets/img/8.bb3bac2d.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><strong>堆疊式 RNNs 通常比單層網絡表現更好</strong>： <ul><li>網絡在不同層次上誘導不同程度的抽象表示。</li><li>堆疊網絡的初始層會產生有用的抽象表示，這些表示可以為後續層提供支持。</li></ul></li><li><strong>然而，隨著堆疊層數增加，訓練成本也會快速上升</strong>。</li></ul><h3 id="雙向-rnns" tabindex="-1"><a class="header-anchor" href="#雙向-rnns"><span>雙向 RNNs</span></a></h3><ul><li><strong>在許多應用中，RNN 需要訪問整個輸入序列</strong>。</li><li><strong>引入了雙向 RNN</strong>： <ul><li>它結合了兩個獨立的雙向 RNN，一個從頭到尾處理輸入，另一個則從尾到頭處理。</li></ul></li></ul><figure><img src="/blog/assets/img/9.3e1b857c.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="用於序列分類的雙向-rnns" tabindex="-1"><a class="header-anchor" href="#用於序列分類的雙向-rnns"><span>用於序列分類的雙向 RNNs</span></a></h3><ul><li><strong>來自前向和後向傳遞的最終隱藏單元結合在一起，表示整個序列</strong>。</li><li><strong>這個結合的表示作為後續分類器的輸入</strong>。</li></ul><figure><img src="/blog/assets/img/10.1b1371a1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="總結" tabindex="-1"><a class="header-anchor" href="#總結"><span>總結</span></a></h2><ul><li><strong>語言模型（LM）</strong>：為詞語序列賦予概率的模型。</li><li><strong>N-gram 語言模型（LM）</strong>：傳統的統計語言模型。</li></ul><h3 id="嵌入方法" tabindex="-1"><a class="header-anchor" href="#嵌入方法"><span>嵌入方法：</span></a></h3><ul><li><strong>稀疏向量（Sparse vectors）</strong>：使用上下文來編碼詞嵌入。（如 TF-IDF, PPMI）</li><li><strong>稠密向量（Dense vectors）</strong>：應用自監督學習來生成。（如 Word2Vec, Contextualized Embeddings）</li></ul><h3 id="神經語言模型-neural-lms" tabindex="-1"><a class="header-anchor" href="#神經語言模型-neural-lms"><span>神經語言模型（Neural LMs）：</span></a></h3><ul><li><strong>FFN</strong>：全連接的稠密神經網絡。</li><li><strong>RNN</strong>：循環神經網絡，儲存時間序列的隱藏狀態。</li></ul><!-- ## 總結 

### How

把文章切句子，找詞性 (主詞 名詞 動詞) by 語言學，可以理解句子
對於古老方式這很難，不過現在有 GPT 那些的



### Word sense ambiguity 

有多種意思的話，怎麼理解? 

ex: Watch for kids ? 
是小孩手錶 or 注意小孩活動告示? 

- 很有趣 back Translation 
    ex:  把中文翻英文 再把英文翻中文 大概有一半會不一樣
    
## 舊方法

用一些方法找出關鍵字，然後分類 (example: 好/壞)。

![](https://hackmd.io/_uploads/r1RzDmepA.png)

像這張圖，那如果出現了紅色的 "新鮮" 和 "價位低"，要怎麼知道是好還是壞呢? 又或者是罕見詞出現呢？

希望比較彈性一點

### One-hot encoding 


![](https://hackmd.io/_uploads/H1i-umla0.png)

壞處: 
- 必須要同樣字才行
- 向量太大了，效率很有問題

但以前沒辦法只能這樣做

### Word embedding(dense space)

(2000~2014) 左右。

所以需要壓縮一下空間。 使向量短，且彼此間可能有所關連。

![](https://hackmd.io/_uploads/HysBuQgaR.png)

使之後如果遇到比較不一樣的字詞，可以評估到底是哪一種分類。

### Synonymy and Polysemy

因為語言有

Synonymy (同義詞)

Polysemy (一詞多義)

可能會產生模糊的狀況

### Concept matching v.s. term matching 

不同領域間會使用不同的字詞
(計程車，出租車)

(智慧型行動裝置，手機)

或者是專有名詞 ex (COVID-19，新冠病毒，武漢肺炎)

### How do we represent the meaning of a word 

wordnet 有建立一些資料集合

![](https://hackmd.io/_uploads/HybAKmgpR.png)
![](https://hackmd.io/_uploads/Bk5ec7lTR.png)

但後來發現建立不完，所以其實也不太好用。

### Continuous, Distributed 

![](https://hackmd.io/_uploads/rkxHcmxTR.png)



### Language Model

(Andrey Markov, Claude Shannon)
: 大語言模型是一個接龍遊戲

GPT 只知道接龍，不知道語意 (但其實不太對)

轉成 vector space 比較 dense 就可算出彼此間的相關性，不須處理比較長的 vector

ex: RAG LLM 

但也造成了他學會了一些歧視 (例如 男生醫生 女生護士)。

![](https://hackmd.io/_uploads/SkJrnXx6A.png)


### Distributional Hypothesis

相似的詞會在類似的地方

ex: The cat licked its fur 
The dog licked its fur

但不會出現 eat run bite 或者 wheel truck 

大型語言就是這樣靠自我監督，慢慢把彼此間關聯。

其實之前 NLP 的時候就有了。 1990 

- 有出現不見得是相關
- 沒出現不見得是相關

LSI 用統計方式，再用 SVD 把重要成份取出來，但現在這個方法不好。

- 矩陣很大 ex: 100000\*100000 跑 SVD 很麻煩
- 運算量太高，不太適用
- 不過感覺很厲害，在那時候是很厲害的一種想法

### Probabilistic Model

![](https://hackmd.io/_uploads/By0AyVlTR.png)

用條件機率的方式來做處理

> 感覺有點類似馬可夫過程

### Model Parameters 

![](https://hackmd.io/_uploads/r1bvXVeaC.png)


- 有個前提，如果一個類神經網路，可以透過加加減減得到後面的文字。懂得文字的意思，那中間的幾百個數字是不是能很清楚代表文字的意思？

很像 GPT 背了一堆書，你給他開頭，他回答了一串文字，那這樣他是不懂嗎？ 好像也說不過去，但你說他懂，但他好像只是會背書。

### What vectors should I use -- it depends 

![](https://hackmd.io/_uploads/B14tL4e6A.png)


 --></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/Hippotumux/blog/edit/main/src/Posts/NLP/NLP3.md" aria-label="在 GitHub " rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub <!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><span class="vp-meta-info" data-allow-mismatch="text">2/24/2025, 3:41:58 AM</span></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: st6805972@gmail.com">hippotumux</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/blog/Posts/NLP/NLP2.html" aria-label="NLP 簡介II" iconsizing="both"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><!---->NLP 簡介II</div></a><a class="route-link auto-link next" href="/blog/Posts/NLP/NLP4.html" aria-label="NLP Sequence-to-sequnce Model" iconsizing="both"><div class="hint">Next<span class="arrow end"></span></div><div class="link">NLP Sequence-to-sequnce Model<!----></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">Hippotumux's Blog</div><div class="vp-copyright">Copyright © 2025 Hippotumux </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/blog/assets/js/runtime~app.56e075d7.js" defer></script><script src="/blog/assets/js/408.dd242f5f.js" defer></script><script src="/blog/assets/js/app.1a083678.js" defer></script>
  </body>
</html>

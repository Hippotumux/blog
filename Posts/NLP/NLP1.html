<!doctype html>
<html lang="en-US" data-theme="dark">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.19" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.71" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://hippotumux.github.io/blog/blog/Posts/NLP/NLP1.html"><meta property="og:site_name" content="Hippotumux's Blog"><meta property="og:title" content="NLP 簡介"><meta property="og:description" content="高宏宇 - 教授 生成式 AI (Generative Artificial Intelligence) 生成式 AI 是一類算法的總稱，這些算法能夠創建新穎的內容。這類算法（例如 ChatGPT）可以應用於多種形式的內容生成，包括： 音頻：生成新的音樂作品、聲音效果等。 程式代碼：自動編寫或優化程式代碼。 圖像：創建新的視覺藝術作品或照片般真實的圖片..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-02-23T16:47:36.000Z"><meta property="article:tag" content="note"><meta property="article:tag" content="theorem"><meta property="article:published_time" content="2024-10-02T00:00:00.000Z"><meta property="article:modified_time" content="2025-02-23T16:47:36.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"NLP 簡介","image":[""],"datePublished":"2024-10-02T00:00:00.000Z","dateModified":"2025-02-23T16:47:36.000Z","author":[{"@type":"Person","name":"Hippotumux","url":"https://hippotumux.github.io/blog/"}]}</script><link rel="icon" href="Mylogo.jpg"><title>NLP 簡介 | Hippotumux's Blog</title><meta name="description" content="高宏宇 - 教授 生成式 AI (Generative Artificial Intelligence) 生成式 AI 是一類算法的總稱，這些算法能夠創建新穎的內容。這類算法（例如 ChatGPT）可以應用於多種形式的內容生成，包括： 音頻：生成新的音樂作品、聲音效果等。 程式代碼：自動編寫或優化程式代碼。 圖像：創建新的視覺藝術作品或照片般真實的圖片...">
    <link rel="stylesheet" href="/blog/assets/css/styles.2ad513e2.css">
    <link rel="preload" href="/blog/assets/js/runtime~app.752d0962.js" as="script"><link rel="preload" href="/blog/assets/css/styles.2ad513e2.css" as="style"><link rel="preload" href="/blog/assets/js/408.dd242f5f.js" as="script"><link rel="preload" href="/blog/assets/js/app.dbd46117.js" as="script">
    <link rel="prefetch" href="/blog/assets/js/Posts_PaperRead_KAN.html.46a9fa91.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_RL_markov_decision.html.5a50040d.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_NN.html.bd43b13d.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_ML2.html.638b97ba.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_ML1.html.601229bf.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP4.html.2feb6cf3.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP2.html.8005f102.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP3.html.5938d056.js" as="script"><link rel="prefetch" href="/blog/assets/js/8300.2a39983f.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_RL_QL_snake.html.9cbe1804.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_markdown.html.b30e427e.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_NLP1.html.4773eae2.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_ML0.html.9a117ce9.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_RAG.html.0d6a7314.js" as="script"><link rel="prefetch" href="/blog/assets/js/intro.html.fa0bbf18.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_page.html.70d09695.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_layout.html.eb3f5b24.js" as="script"><link rel="prefetch" href="/blog/assets/js/cool.html.f4d72207.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_disable.html.f0336708.js" as="script"><link rel="prefetch" href="/blog/assets/js/index.html.6e782bed.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_encrypt.html.3f85a1ef.js" as="script"><link rel="prefetch" href="/blog/assets/js/demo_index.html.83124deb.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_index.html.f07d3321.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_使用指南_index.html.2f018673.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_使用指南_index.html.09488c89.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_页面配置_index.html.87bbd16a.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_指南_index.html.8b205227.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_probability_index.html.9376bbbf.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_diary_index.html.c81e785c.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_加密_index.html.8f34de35.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_禁用_index.html.cd7492a1.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_布局_index.html.5e2962b5.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_markdown_index.html.857b4a85.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_nlp_index.html.1bd6bf70.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_theorem_index.html.59252ad7.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_ml_index.html.7e5a8f93.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_nn_index.html.6359af08.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_python_index.html.2fc5e11f.js" as="script"><link rel="prefetch" href="/blog/assets/js/404.html.ae79f368.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_note_index.html.70218866.js" as="script"><link rel="prefetch" href="/blog/assets/css/6198.styles.ac85cc55.css" as="style"><link rel="prefetch" href="/blog/assets/js/tag_imp_index.html.6b2baddd.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_kan_index.html.c73872ee.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_pdf_index.html.0cc415f9.js" as="script"><link rel="prefetch" href="/blog/assets/js/category_index.html.d8997b45.js" as="script"><link rel="prefetch" href="/blog/assets/js/timeline_index.html.9d6bff97.js" as="script"><link rel="prefetch" href="/blog/assets/js/article_index.html.037bcb43.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_PaperRead_index.html.4c469e6a.js" as="script"><link rel="prefetch" href="/blog/assets/js/tag_index.html.685e29b4.js" as="script"><link rel="prefetch" href="/blog/assets/js/star_index.html.393200c5.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_NLP_index.html.43126e4a.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_ML_index.html.df9d0fe8.js" as="script"><link rel="prefetch" href="/blog/assets/js/Posts_RL_index.html.50262dca.js" as="script"><link rel="prefetch" href="/blog/assets/css/1409.styles.8ef4570a.css" as="style">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/blog/" aria-label="Take me home"><img class="vp-nav-logo" src="/blog/assets/images/Mylogo.jpg" alt><!----><span class="vp-site-name hide-in-pad">Hippotumux&#39;s Blog</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/" aria-label="Home" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" height="1em" sizing="height"></iconify-icon><!--]-->Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/demo/" aria-label="Projects" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:book-2-outline" height="1em" sizing="height"></iconify-icon><!--]-->Projects<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link route-link-active auto-link" href="/blog/Posts/" aria-label="Posts" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:add-notes-outline" height="1em" sizing="height"></iconify-icon><!--]-->Posts<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/intro.html" aria-label="About Me" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:family-star" height="1em" sizing="height"></iconify-icon><!--]-->About Me<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/Hippotumux/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!----><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/" aria-label="Home" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->Home<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><iconify-icon class="vp-icon" icon="material-symbols:book-2-outline" width="1em" height="1em" sizing="both"></iconify-icon><a class="route-link auto-link vp-sidebar-title no-external-link-icon" href="/blog/demo/" aria-label="Projects" iconsizing="both"><!---->Projects<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/markdown.html" aria-label="Markdown 展示" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-brands:markdown" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->Markdown 展示<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/layout.html" aria-label="你好" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:object-group" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->你好<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/page.html" aria-label="页面配置" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->页面配置<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/disable.html" aria-label="布局与功能禁用" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:gears" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->布局与功能禁用<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/demo/encrypt.html" aria-label="密码加密的文章" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:lock" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->密码加密的文章<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable active"><iconify-icon class="vp-icon" icon="material-symbols:add-notes-outline" width="1em" height="1em" sizing="both"></iconify-icon><a class="route-link route-link-active auto-link vp-sidebar-title no-external-link-icon" href="/blog/Posts/" aria-label="Posts" iconsizing="both"><!---->Posts<!----></a><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">ML</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">NLP</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/blog/Posts/NLP/NLP1.html" aria-label="NLP 簡介" iconsizing="both"><!---->NLP 簡介<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/NLP2.html" aria-label="NLP 簡介II" iconsizing="both"><!---->NLP 簡介II<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/NLP3.html" aria-label="NLP word embedding and RNN" iconsizing="both"><!---->NLP word embedding and RNN<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/NLP4.html" aria-label="NLP Sequence-to-sequnce Model" iconsizing="both"><!---->NLP Sequence-to-sequnce Model<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/Posts/NLP/RAG.html" aria-label="RAG 介紹" iconsizing="both"><!---->RAG 介紹<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Paper Read</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">RL</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/intro.html" aria-label="About Me" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="material-symbols:family-star" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->About Me<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->NLP 簡介</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://hippotumux.github.io/blog/" target="_blank" rel="noopener noreferrer">Hippotumux</a></span><span property="author" content="Hippotumux"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">October 2, 2024</span><meta property="datePublished" content="2024-10-02T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 10 min</span><meta property="timeRequired" content="PT10M"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color2 clickable" role="navigation">NLP</span><!--]--><meta property="articleSection" content="NLP"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color4 clickable" role="navigation">note</span><span class="page-tag-item color0 clickable" role="navigation">theorem</span><!--]--><meta property="keywords" content="note,theorem"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><!--[--><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#生成式-ai-generative-artificial-intelligence">生成式 AI (Generative Artificial Intelligence)</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#原理">原理</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#圖像生成技術">圖像生成技術</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#gan-生成對抗網路">GAN (生成對抗網路)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#diffusion-model-擴散模型">Diffusion Model (擴散模型)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#那文字呢">那文字呢？</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#nlp-自然語言處理">NLP (自然語言處理)</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#nlp-levels">NLP Levels</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#example">example</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#電腦該如何了解">電腦該如何了解？</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#nlp-歷史">NLP 歷史</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#語言模型-language-model">語言模型 (Language Model)</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#學習">學習</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--]--><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="nlp-簡介" tabindex="-1"><a class="header-anchor" href="#nlp-簡介"><span>NLP 簡介</span></a></h1><p>高宏宇 - 教授</p><h2 id="生成式-ai-generative-artificial-intelligence" tabindex="-1"><a class="header-anchor" href="#生成式-ai-generative-artificial-intelligence"><span>生成式 AI (Generative Artificial Intelligence)</span></a></h2><p>生成式 AI 是一類算法的總稱，這些算法能夠創建新穎的內容。這類算法（例如 ChatGPT）可以應用於多種形式的內容生成，包括：</p><ul><li>音頻：生成新的音樂作品、聲音效果等。</li><li>程式代碼：自動編寫或優化程式代碼。</li><li>圖像：創建新的視覺藝術作品或照片般真實的圖片。</li><li>文字：寫作文章、翻譯或其他任何形式的文字內容。</li><li>視頻：生成新的動畫或模擬實景視頻。</li><li>其它可能的應用包括遊戲內容生成、模擬對話等。</li></ul><!-- more --><h3 id="原理" tabindex="-1"><a class="header-anchor" href="#原理"><span>原理</span></a></h3><p>生成式 AI 通常基於深度學習模型，這些模型通過學習大量數據來理解和模擬特定形式的內容。例如，聊天機器人如 ChatGPT 通過學習大量文本數據來生成人類般的回答。生成型模型不僅能夠模仿學習到的內容類型，還能創作出全新的未見過的內容。</p><p>這類人工智慧的應用非常廣泛，不僅可以提高創意產業的效率，還能在教育、娛樂等領域開創新的可能性。</p><h3 id="圖像生成技術" tabindex="-1"><a class="header-anchor" href="#圖像生成技術"><span>圖像生成技術</span></a></h3><ol><li>GAN（生成對抗網絡）：透過對抗過程，讓生成網絡和判斷網絡相互競爭，進而生成高質量的圖像。 <ul><li>NVIDIA：開發了StyleGAN2，專注於生成高解析度和高質量的圖像。</li><li>DeepMind：開發了BigGAN，用於生成大規模且細節豐富的圖像。</li></ul></li><li>Stable Diffusion：Stability AI 開發的技術，可以根據文字描述生成圖像。 <ul><li>StabilityAI：推出了Stable Diffusion，支援文本到圖像的生成。<br> Transformer技術：<br> OpenAI：開發了DALL-E和CLIP，專門從文本描述生成對應的圖像。<br> LeepMotion：推出了Midjourney，是一個新的圖像生成工具。<br> 文本生成技術</li></ul></li><li>GPT（生成預訓練變換器）：使用深度學習的變換器結構來生成連貫和有意義的文本。 <ul><li>OpenAI：推出了ChatGPT和GPT-4等系列產品。</li><li>Meta：開發了LLaMA2，一個高性能的語言模型。</li><li>Google：提供了T5、XLNet、PaLM等多種語言模型。<br> 語音生成技術</li></ul></li><li>GAN和VAE（變分自編碼器）：用於生成自然語音。 <ul><li>Google：WaveNet，用於生成接近真實的語音。</li><li>OpenAI：WaveGlow，一種結合了GAN和VAE的語音合成技術。<br> 程式代碼生成技術</li><li>Seq2Seq和GPT：自動編寫或優化程式代碼。<br> OpenAI：推出了Copilot，幫助開發者通過建議代碼片段來提高編碼效率。</li></ul></li></ol><h3 id="gan-生成對抗網路" tabindex="-1"><a class="header-anchor" href="#gan-生成對抗網路"><span>GAN (生成對抗網路)</span></a></h3><p>Generative Adversarial Network</p><figure><img src="/blog/assets/img/1.8c20a647.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li>D-維噪聲向量<br> 這是GAN的輸入，通常是隨機生成的，目的是提供初始的「種子」，從而讓生成網絡（Generator Network）可以開始生成假的圖像。</li><li>生成網絡（Generator Network）<br> 這是GAN中的核心部分之一，負責接收D-維噪聲向量作為輸入，並通過學習如何模仿真實圖像的分布，生成假的圖像（Fake Images）。生成網絡的目標是創造足夠逼真的圖像，以至於鑑別網絡（Discriminator Network）難以區分真假。</li><li>假圖像（Fake Images）<br> 由生成網絡產生的圖像，這些圖像是對真實圖像的模仿。初始階段，這些假圖像的質量可能不高，但隨著訓練的進行，它們會逐漸變得更加逼真。</li><li>真實圖像（Real Images）<br> 這些是從數據集中提取的實際圖像，用於訓練鑑別網絡，使其能夠識別出哪些是真實的，哪些是由生成網絡創造的假圖像。</li><li>鑑別網絡（Discriminator Network）<br> GAN的另一核心部分，它的任務是區分輸入的圖像是真實的還是假的。它接收來自生成網絡的假圖像和真實圖像作為輸入，並試圖正確標記它們。</li><li>預測標籤（Predicted Labels）<br> 鑑別網絡對圖像進行分類後的結果，表明每個圖像是「真實」還是「假」的。這些預測用於訓練生成網絡和鑑別網絡，使生成網絡能夠生成更逼真的圖像，而鑑別網絡則變得更擅長於識別假圖像。</li></ol><figure><img src="/blog/assets/img/2.e7c29b32.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="diffusion-model-擴散模型" tabindex="-1"><a class="header-anchor" href="#diffusion-model-擴散模型"><span>Diffusion Model (擴散模型)</span></a></h3><figure><img src="/blog/assets/img/3.a5d10c7e.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>正向擴散（Forward Diffusion） <ul><li>過程：這是擴散模型的第一階段，其中原始圖像逐步被加入噪聲，直至完全變為噪聲。如圖所示，從清晰的貓的圖像開始，逐步增加噪聲，最後變成不可辨識的噪點圖案。</li><li>目的：增加噪聲的過程是為了建立一個從“清晰圖像到純噪聲”的轉化映射，這一映射在模型訓練中是非常重要的。</li></ul></li><li>反向擴散（Reverse Diffusion） <ul><li>過程：這是擴散模型的第二階段，通過從加噪後的圖像中逐步去除噪聲，逐漸恢復到原始或接近原始的圖像。這個過程是模型生成新圖像的核心。</li><li>技術：通常使用隨機微分方程（SDE）或概率流（Probability Flow ODE）等數學工具來指導這一逆過程，確保噪聲能夠按照預定的路徑被有效去除。<br> 應用</li></ul></li></ul><p>這種模型在生成圖像、視頻、聲音等多種媒體內容方面具有極高的應用價值，因為它可以從隨機噪聲中創建高質量的、結構化的輸出。這種模型的一大優點是生成過程的可控性和生成結果的多樣性。</p><h3 id="那文字呢" tabindex="-1"><a class="header-anchor" href="#那文字呢"><span>那文字呢？</span></a></h3><h2 id="nlp-自然語言處理" tabindex="-1"><a class="header-anchor" href="#nlp-自然語言處理"><span>NLP (自然語言處理)</span></a></h2><p>Natural Language Processing</p><ul><li>希望讓電腦理解人類語言</li><li>有一門課為計算語言學(基本上是一樣的事情)</li></ul><h3 id="nlp-levels" tabindex="-1"><a class="header-anchor" href="#nlp-levels"><span>NLP Levels</span></a></h3><ol><li>形態學（Morphology）<br> 這一層次涉及到詞語的結構及基於詞根、前綴、後綴和詞形變化的詞形構造。<br> 前綴/後綴：分析詞語的開頭和結尾，以了解它們的意義或語法功能。<br> 詞形還原/詞幹提取：將詞語還原到基本形式，或提取詞幹以分析基本的意義。<br> 拼寫檢查：檢測和更正文本中的拼寫錯誤。</li><li>句法（Syntax）<br> 這一層次涉及到句子的結構，如何將詞語組合成合乎語法的句子。<br> 詞性標注：確定文本中每個詞的詞性（如名詞、動詞等）。<br> 句法樹：創建表示句子結構的樹狀圖，顯示詞語之間的層次關係。<br> 依存句法樹：構建展示詞語間依賴關係的圖，通常用於理解句子中各個元素的語法功能。</li><li>語義（Semantics）<br> 這一層次處理詞語和句子的意義，如何從文本中提取意義。<br> 命名實體識別/規範化：識別文本中的專有名詞（如人名、地點等），並將其規範化為標準形式。<br> 關係提取：從文本中識別實體之間的關係。<br> 詞義消歧：確定在給定上下文中詞語的確切意義。</li><li>語用學（Pragmatics）<br> 這一層次涉及到語言的實際使用情境和語言的功能。<br> 共指消解：識別文本中提到的多個表達是指向同一實體。<br> 主題分段：將文本分割成不同的主題區塊。<br> 摘要：提取文本的主要內容，生成摘要。</li></ol><h3 id="example" tabindex="-1"><a class="header-anchor" href="#example"><span>example</span></a></h3><p>給一個新聞，你需要做什麼？</p><ul><li>分類</li><li>語言</li><li>分辨假新聞?</li><li>摘要</li><li>等等</li></ul><h3 id="電腦該如何了解" tabindex="-1"><a class="header-anchor" href="#電腦該如何了解"><span>電腦該如何了解？</span></a></h3><p>Ｑ：曾有一項調查發現，很多員工生病的時候不敢請假，因為他們擔心<br> 老闆會不高興，覺得他們沒有責任感。有人認為，員工會這麼想是公司<br> 的責任。一個好的公司應該能照顧員工，而不是讓他們拿健康去換錢。<br> 因此，讓員工有幸福感，應該是未來企業努力的方向。<br> 這篇文章說了什麼內容？</p><p>1.老闆應該給員工多一點兒假<br> 2.常關心別人的人更有責任感<br> 3.對公司有意見要勇敢說出來<br> 4.照顧身體比認真工作更重要</p><p><img src="/blog/assets/img/4.e4d5184e.png" alt="" loading="lazy"><br> (by 科技大擂台, 2017)</p><h3 id="nlp-歷史" tabindex="-1"><a class="header-anchor" href="#nlp-歷史"><span>NLP 歷史</span></a></h3><figure><img src="/blog/assets/img/5.f74c14bd.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li>2000年代初：基礎技術的發展 <ul><li>Yoshua Bengio: 開發了神經機率語言模型，為後續的深度學習模型奠定了基礎。<br> 關鍵技術:</li><li>Parsing tree（句法樹）: 用於分析句子結構。 <ul><li>TF-IDF（詞頻-逆文檔頻率）: 一種評估詞語重要性的統計方法。</li><li>Bag of word（詞袋模型）: 處理文本數據的基本方法，忽略語序和語法結構。</li><li>Vector space（向量空間）: 將文本數據轉換為向量，以便用於各種機器學習算法。</li></ul></li></ul></li><li>2013年：語義理解的突破 <ul><li>Google: word2vec: 這是一種創新的詞嵌入方法，用於將詞語轉換成密集的向量，以更好地捉摸詞語間的語義關係。</li></ul></li><li>2018年：變革性的變換器架構（Transformer Era） <ul><li>BERT（Bidirectional Encoder Representations from Transformers）: Google 開發的模型，利用雙向變換器架構，大幅提升了機器對語言的理解能力。</li></ul></li><li>2019 - 2020年：生成預訓練變換器的進一步發展 <ul><li>GPT-2及GPT-3: OpenAI 開發的模型，具有強大的文本生成能力，能夠創建高度連貫和有說服力的文本。</li></ul></li><li>2022 - 2024年：指令型生成預訓練變換器和聊天型GPT <ul><li>Instruct-GPT和ChatGPT: 這些模型設計用於更好地理解和執行用戶指令，並與用戶進行自然的對話。</li><li>LLMs（Large Language Models）: 指大規模語言模型，這些模型由於其巨大的參數量和廣泛的訓練數據，展現出卓越的語言處理能力。</li></ul></li></ol><h2 id="語言模型-language-model" tabindex="-1"><a class="header-anchor" href="#語言模型-language-model"><span>語言模型 (Language Model)</span></a></h2><figure><img src="/blog/assets/img/6.5fc27be1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><blockquote><p>Andrey Markov 就是在 1913 年發明馬可夫鏈的人。</p></blockquote><!-- 熵用於描述一個信息源發出的信息量的平均不確定性。在更技術性的語言中，熵是衡量一個隨機變數不確定性的度量。如果一個隨機變數的可能狀態多且分布均勻，它的熵就高；相反，如果這個隨機變數的取值非常確定，它的熵就低。 --><h3 id="學習" tabindex="-1"><a class="header-anchor" href="#學習"><span>學習</span></a></h3><figure><img src="/blog/assets/img/7.17880057.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>轉移學習（Transfer Learning）</p><ul><li><p>轉移學習是一種機器學習方法，它允許模型首先在一個大規模的數據集上預訓練，然後在特定的任務上進行微調。這樣可以將在一個任務上學到的知識應用到其他相似的任務中。</p><ol><li>預訓練（Pre-training） <ul><li>過程：通常在大規模的數據集上進行，使用的是自監督學習方法，如掩碼語言模型（Masked Language Model, MLM）。</li><li>例子：如圖中的(a)部分所示，句子中的某些詞（如 &quot;painful&quot;）被隨機遮蔽（表示為 [MASK]），模型需預測這些被遮蔽的詞，這有助於模型學習語言的深層特征和語境。</li></ul></li><li>微調（Fine-tuning） <ul><li>過程：預訓練完成後，模型會在特定的任務上進行微調。這通常涉及較小的數據集，這些數據集是針對特定任務標記的。</li><li>例子：如圖中的(b)部分所示，使用特定於任務的頭部（例如情感分類）進行訓練，根據輸入的句子來預測特定的輸出（如標籤“positive”或“negative”）</li></ul></li></ol></li></ul><p>這種預訓練和微調的方法非常高效，因為預訓練的模型已經學會了許多有用的語言表示，微調階段則讓模型能夠專注於特定任務的細節，從而在特定的應用中達到更好的性能。這一方法在自然語言處理領域非常流行，廣泛用於各種語言理解任務中。</p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/Hippotumux/blog/edit/main/src/Posts/NLP/NLP1.md" aria-label="在 GitHub " rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub <!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><span class="vp-meta-info" data-allow-mismatch="text">2/23/2025, 4:47:36 PM</span></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: st6805972@gmail.com">hippotumux</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><!----><a class="route-link auto-link next" href="/blog/Posts/NLP/NLP2.html" aria-label="NLP 簡介II" iconsizing="both"><div class="hint">Next<span class="arrow end"></span></div><div class="link">NLP 簡介II<!----></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">Hippotumux's Blog</div><div class="vp-copyright">Copyright © 2025 Hippotumux </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/blog/assets/js/runtime~app.752d0962.js" defer></script><script src="/blog/assets/js/408.dd242f5f.js" defer></script><script src="/blog/assets/js/app.dbd46117.js" defer></script>
  </body>
</html>

"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[656],{6262:(s,a)=>{a.A=(s,a)=>{const n=s.__vccOpts||s;for(const[s,t]of a)n[s]=t;return n}},7901:(s,a,n)=>{n.r(a),n.d(a,{comp:()=>c,data:()=>d});var t=n(641);const l=n.p+"assets/img/1.6c004dd7.png",e=n.p+"assets/img/2.21cbfb29.png",i=n.p+"assets/img/3.c0d2ce3e.png",p=n.p+"assets/img/4.90c09f88.png",m=n.p+"assets/img/6.e6517413.png",r=n.p+"assets/img/8.bb3bac2d.png",o=n.p+"assets/img/9.3e1b857c.png",g=n.p+"assets/img/10.1b1371a1.png",h={},c=(0,n(6262).A)(h,[["render",function(s,a){return(0,t.uX)(),(0,t.CE)("div",null,[a[0]||(a[0]=(0,t.Fv)('<h1 id="nlp-word-embedding-and-rnn" tabindex="-1"><a class="header-anchor" href="#nlp-word-embedding-and-rnn"><span>NLP word embedding and RNN</span></a></h1><p>高宏宇 - 教授</p><h2 id="n-grams" tabindex="-1"><a class="header-anchor" href="#n-grams"><span>N-grams</span></a></h2><p><strong>N-gram</strong> 是一個包含 n 個單詞的序列。通過不同數量的單詞組合，N-gram 模型可以捕捉語言中的上下文關係。</p><ul><li><strong>1-gram（unigram）</strong>：單詞序列中的每個單詞單獨作為一個項目，例如： <ul><li>&quot;please&quot;</li><li>&quot;turn&quot;</li><li>&quot;your&quot;</li><li>&quot;homework&quot;</li></ul></li><li><strong>2-gram（bigram）</strong>：兩個連續的單詞組合，例如： <ul><li>&quot;please turn&quot;</li><li>&quot;turn your&quot;</li><li>&quot;your homework&quot;</li></ul></li><li><strong>3-gram（trigram）</strong>：三個連續的單詞組合，例如： <ul><li>&quot;please turn your&quot;</li><li>&quot;turn your homework&quot;</li></ul></li><li><strong>N-gram</strong> 模型可擴展至更高的數值，例如 4-gram、n-gram 等。</li></ul>',5)),(0,t.Q3)(" more "),a[1]||(a[1]=(0,t.Fv)('<h3 id="n-gram-語言模型" tabindex="-1"><a class="header-anchor" href="#n-gram-語言模型"><span>N-gram 語言模型</span></a></h3><p>我們可以使用一種簡單的統計方法來構建 N-gram 語言模型，該模型依賴於計算特定 n-gram 在語料庫中出現的次數。該方法假設詞語的出現僅取決於前 n-1 個詞，而不考慮更長距離的語境。</p><h3 id="bi-gram-模型示例" tabindex="-1"><a class="header-anchor" href="#bi-gram-模型示例"><span>Bi-gram 模型示例</span></a></h3><p>假設有以下句子：</p><ul><li>&quot;I want to eat lunch.&quot;</li><li>&quot;I want to eat Chinese food.&quot;</li><li>&quot;I don&#39;t want to spend time cooking.&quot;</li></ul><p>在 Bi-gram 模型中，我們需要統計每個二元詞組出現的次數。例如：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mtext>I want</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">C(\\text{I want}) = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord text"><span class="mord">I want</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mtext>want to</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">C(\\text{want to}) = 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord text"><span class="mord">want to</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mtext>spend time</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">C(\\text{spend time}) = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord text"><span class="mord">spend time</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></li></ul><p>這樣的模型可以通過統計不同 n-gram 的出現次數來預測接下來的單詞，從而達到基本的語言生成和建模的效果。</p><h3 id="模型比較" tabindex="-1"><a class="header-anchor" href="#模型比較"><span>模型比較</span></a></h3><h4 id="_1-n-gram-語言模型" tabindex="-1"><a class="header-anchor" href="#_1-n-gram-語言模型"><span>1. <strong>N-Gram 語言模型</strong></span></a></h4><ul><li><strong>定義</strong>：一種純粹的統計模型，基於先前出現的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 個詞來預測接下來的詞，並計算詞序列的出現概率。</li><li><strong>特點</strong>：N-Gram 模型簡單易用，但它僅考慮有限的上下文，難以捕捉長距離依賴。</li></ul><h4 id="_2-神經網絡語言模型-neural-language-models" tabindex="-1"><a class="header-anchor" href="#_2-神經網絡語言模型-neural-language-models"><span>2. <strong>神經網絡語言模型（Neural Language Models）</strong></span></a></h4><ul><li><strong>定義</strong>：利用神經網絡來預測詞語序列的可能性，通過學習詞向量來捕捉語義和語法關係。</li><li><strong>特點</strong>：相比 N-Gram 模型，神經網絡語言模型能更好地處理長距離依賴，並且能學習到更豐富的語言結構。</li></ul><h2 id="困惑度-perplexity-ppl" tabindex="-1"><a class="header-anchor" href="#困惑度-perplexity-ppl"><span>困惑度（Perplexity, PPL）</span></a></h2><p><strong>困惑度（Perplexity, PPL）</strong> 是一種量化標準，用來評估語言模型的能力或效果。</p><ul><li><p><strong>定義</strong>：給定詞語序列和一個 N-gram 模型，困惑度是通過以下公式計算的：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>P</mi><mi>L</mi><mo>=</mo><msup><mn>2</mn><mrow><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mtext> </mtext><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">PPL = 2^{-\\frac{1}{N} \\sum_{i=1}^N \\log_2 P(w_i \\mid w_{i-1}, w_{i-2}, \\dots)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">PP</span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0564em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0564em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mop mtight"><span class="mtight">l</span><span class="mtight">o</span><span class="mtight" style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span style="top:-2.2341em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2659em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mrel mtight">∣</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="minner mtight">…</span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></p></li><li><p><strong>解釋</strong>：困惑度值越低，表示語言模型的能力越好，能更準確地進行詞語預測。</p></li></ul><h3 id="意義" tabindex="-1"><a class="header-anchor" href="#意義"><span>意義</span></a></h3><ol><li><strong>不確定性的度量</strong>： <ul><li>困惑度量化了模型在進行預測時所面臨的不確定性或不可預測性。</li></ul></li><li><strong>平均分支因子</strong>： <ul><li>困惑度可以視為在每個序列步驟中「平均分支因子」的度量，表示在每一步中模型可能選擇的選項數量。</li></ul></li><li><strong>模型表現的量化</strong>： <ul><li>在語言建模中，困惑度反映了模型對語言規則和結構的理解程度。</li><li>困惑度越低，表明模型對語言的理解和預測能力越好，表明模型能有效捕捉語言模式。</li></ul></li><li><strong>壓縮效率的指標</strong>： <ul><li>困惑度也可以視為模型如何有效壓縮測試數據的度量。</li><li>困惑度越低，表示模型以更高的概率進行預測，減少了不確定性，從而有效地壓縮了信息。</li></ul></li></ol><h2 id="word-embedding" tabindex="-1"><a class="header-anchor" href="#word-embedding"><span>Word Embedding</span></a></h2><p><strong>Word Embedding</strong> 是一種將詞語表示為連續向量的技術，這些向量能夠捕捉詞語之間的語義和語法關係。</p><h4 id="_1-概念空間-concept-space" tabindex="-1"><a class="header-anchor" href="#_1-概念空間-concept-space"><span>1. <strong>概念空間（Concept Space）</strong></span></a></h4><ul><li><strong>定義</strong>：概念空間是指詞嵌入向量所在的向量空間，詞語以稠密向量的形式分布在這個空間中，能夠反映詞語之間的語義相似性。</li></ul><h4 id="_2-詞項向量-term-vector" tabindex="-1"><a class="header-anchor" href="#_2-詞項向量-term-vector"><span>2. <strong>詞項向量（Term Vector）</strong></span></a></h4><ul><li><strong>定義</strong>：詞項向量是指每個詞語對應的一個稠密向量，這些向量用來表示詞語在概念空間中的位置。</li></ul><h4 id="_3-稠密向量-dense-vectors" tabindex="-1"><a class="header-anchor" href="#_3-稠密向量-dense-vectors"><span>3. <strong>稠密向量（Dense Vectors）</strong></span></a></h4><ul><li><strong>定義</strong>：稠密向量嵌入將詞語表示在連續的向量空間中，詞語之間的相似性可以根據向量之間的距離來衡量。</li><li><strong>特性</strong>：語義相似的詞會在空間中靠得更近。</li></ul><h4 id="_4-word2vec" tabindex="-1"><a class="header-anchor" href="#_4-word2vec"><span>4. <strong>Word2Vec</strong></span></a></h4><ul><li><strong>Word2Vec</strong> 是一種常用的詞嵌入方法，它基於上下文來學習詞語的向量表示，能夠捕捉到詞語的語義信息。</li></ul><h4 id="_5-上下文嵌入-contextualized-embeddings" tabindex="-1"><a class="header-anchor" href="#_5-上下文嵌入-contextualized-embeddings"><span>5. <strong>上下文嵌入（Contextualized Embeddings）</strong></span></a></h4><ul><li><strong>定義</strong>：上下文嵌入會根據詞語所處的語境來動態調整詞嵌入向量，讓相同的詞在不同的語境中可以有不同的表示。</li></ul><h3 id="嵌入的特性" tabindex="-1"><a class="header-anchor" href="#嵌入的特性"><span>嵌入的特性</span></a></h3><ol><li><strong>類比關係/關聯相似性</strong><ul><li><p>詞嵌入可以捕捉詞語之間的類比關係。例如：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Washington</mtext><mo>−</mo><mtext>U.S.</mtext><mo>=</mo><mtext>London</mtext><mo>−</mo><mtext>U.K.</mtext></mrow><annotation encoding="application/x-tex">\\text{Washington} - \\text{U.S.} = \\text{London} - \\text{U.K.} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Washington</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">U.S.</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">London</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">U.K.</span></span></span></span></span></span></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Washington</mtext><mo>−</mo><mtext>U.S.</mtext><mo>+</mo><mtext>U.K.</mtext><mo>=</mo><mtext>London</mtext></mrow><annotation encoding="application/x-tex">\\text{Washington} - \\text{U.S.} + \\text{U.K.} = \\text{London} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Washington</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">U.S.</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">U.K.</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">London</span></span></span></span></span></span></p></li></ul></li></ol><h2 id="word2vec" tabindex="-1"><a class="header-anchor" href="#word2vec"><span>Word2Vec</span></a></h2><p><strong>Word2Vec</strong> 是一種詞嵌入技術，通過將詞語表示為向量，捕捉詞語的語義關係。Word2Vec 有兩種主要的模型架構：<strong>Skip-gram</strong> 和 <strong>CBOW</strong>。</p><h4 id="_1-skip-gram" tabindex="-1"><a class="header-anchor" href="#_1-skip-gram"><span>1. <strong>Skip-gram</strong></span></a></h4><ul><li><strong>定義</strong>：使用目標詞來預測其上下文詞語。</li><li><strong>原理</strong>：Skip-gram 模型的目標是給定一個中心詞，預測這個詞周圍的上下文詞語。這使得模型能夠學習到詞與其上下文的語義聯繫，特別適合於處理稀疏數據。</li></ul><h4 id="_2-cbow-continuous-bag-of-words" tabindex="-1"><a class="header-anchor" href="#_2-cbow-continuous-bag-of-words"><span>2. <strong>CBOW（Continuous Bag of Words）</strong></span></a></h4><ul><li><strong>定義</strong>：使用上下文詞來預測目標詞。</li><li><strong>原理</strong>：CBOW 模型的目標是根據周圍的上下文詞語來預測中心詞。這種模型利用上下文信息來預測詞語，對於捕捉局部語境非常有效。</li></ul><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="word2vec-流程" tabindex="-1"><a class="header-anchor" href="#word2vec-流程"><span>Word2Vec 流程</span></a></h3><h4 id="_1-將目標詞與其鄰近的上下文詞作為正樣本" tabindex="-1"><a class="header-anchor" href="#_1-將目標詞與其鄰近的上下文詞作為正樣本"><span>1. <strong>將目標詞與其鄰近的上下文詞作為正樣本</strong></span></a></h4><h4 id="_2-隨機從詞彙表中抽取其他詞作為負樣本" tabindex="-1"><a class="header-anchor" href="#_2-隨機從詞彙表中抽取其他詞作為負樣本"><span>2. <strong>隨機從詞彙表中抽取其他詞作為負樣本</strong></span></a></h4><h4 id="_3-使用邏輯回歸訓練分類器以區分正負樣本" tabindex="-1"><a class="header-anchor" href="#_3-使用邏輯回歸訓練分類器以區分正負樣本"><span>3. <strong>使用邏輯回歸訓練分類器以區分正負樣本</strong></span></a></h4><h4 id="_4-使用學到的權重作為嵌入向量" tabindex="-1"><a class="header-anchor" href="#_4-使用學到的權重作為嵌入向量"><span>4. <strong>使用學到的權重作為嵌入向量</strong></span></a></h4><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="上下文嵌入-contextualized-word-embeddings" tabindex="-1"><a class="header-anchor" href="#上下文嵌入-contextualized-word-embeddings"><span>上下文嵌入（Contextualized Word Embeddings）</span></a></h3><p><strong>上下文嵌入</strong> 是一種將詞嵌入與模型緊密結合的技術。它能根據具體任務的上下文動態調整詞語的表示，使模型能夠更好地理解詞語在不同上下文中的語義。</p><h4 id="_1-嵌入設計為模型的一部分" tabindex="-1"><a class="header-anchor" href="#_1-嵌入設計為模型的一部分"><span>1. <strong>嵌入設計為模型的一部分</strong></span></a></h4><h4 id="_2-根據下游任務調整" tabindex="-1"><a class="header-anchor" href="#_2-根據下游任務調整"><span>2. <strong>根據下游任務調整</strong></span></a></h4><h3 id="神經網絡架構中的嵌入層和語言模型層" tabindex="-1"><a class="header-anchor" href="#神經網絡架構中的嵌入層和語言模型層"><span>神經網絡架構中的嵌入層和語言模型層</span></a></h3><ul><li>在神經網絡架構中，詞嵌入層通常作為輸入層，將詞語轉換為向量。</li><li>語言模型層（LM Layers）則可以進一步處理這些嵌入，捕捉更多的語法和語義信息。</li></ul><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="神經語言模型" tabindex="-1"><a class="header-anchor" href="#神經語言模型"><span>神經語言模型</span></a></h2><p>需要一個模型結構來處理嵌入中的隱藏特徵。神經語言模型利用神經網絡來學習和表示複雜的語言模式。</p><ul><li><strong>FFN（前饋神經網絡，Feedforward Network）</strong></li><li><strong>RNN（循環神經網絡，Recurrent Neural Networks）</strong></li></ul><h3 id="深度學習基礎" tabindex="-1"><a class="header-anchor" href="#深度學習基礎"><span>深度學習基礎</span></a></h3><ol><li><strong>模型（Model）</strong><ul><li>模型指的是用來表示輸入數據與輸出預測之間關係的架構或結構。</li></ul></li><li><strong>優化器（Optimizer）</strong><ul><li>優化器是一種算法，用於在訓練過程中調整模型的參數，以最小化預測值與實際輸出值之間的誤差。</li></ul></li><li><strong>損失函數（Loss Function）</strong><ul><li>損失函數（目標函數）用來衡量模型的預測輸出與真實目標輸出之間的差異。</li></ul></li></ol><h3 id="訓練神經網絡" tabindex="-1"><a class="header-anchor" href="#訓練神經網絡"><span>訓練神經網絡</span></a></h3><p>訓練過程通常包括以下步驟：</p><ol><li><strong>數據準備</strong>：準備訓練和測試數據集。</li><li><strong>模型構建</strong>：使用深度學習框架（TensorFlow, PyTorch 等）構建模型。</li><li><strong>損失函數定義</strong>：選擇適當的損失函數。（交叉熵，Logloss 等）</li><li><strong>優化器選擇</strong>：選擇合適的優化算法。（Adam, SGD 等）</li><li><strong>模型訓練</strong>：使用訓練數據集訓練模型。</li><li><strong>模型評估</strong>：使用測試數據集評估訓練好的模型。（F1, LCS 等）</li></ol><h3 id="激活函數" tabindex="-1"><a class="header-anchor" href="#激活函數"><span>激活函數</span></a></h3><ul><li>使用激活函數的核心思想是為神經網絡引入非線性。</li><li>神經網絡模型旨在避免最終的處理階段僅僅是輸入的線性變換，從而使模型能夠在複雜問題上有良好的表現。</li><li>常用的激活函數包括：Sigmoid, ReLU, tanh, GeLU。</li></ul><h2 id="ffn" tabindex="-1"><a class="header-anchor" href="#ffn"><span>FFN</span></a></h2><h3 id="前饋神經網絡-ffn" tabindex="-1"><a class="header-anchor" href="#前饋神經網絡-ffn"><span>前饋神經網絡（FFN）</span></a></h3><ul><li>前饋神經網絡（FFN）是一種多層的前饋網絡，網絡中的單元之間的連接沒有循環。<br><img src="'+p+'" alt="" loading="lazy"></li></ul><h3 id="前饋神經網絡-ffn-的缺點" tabindex="-1"><a class="header-anchor" href="#前饋神經網絡-ffn-的缺點"><span>前饋神經網絡（FFN）的缺點</span></a></h3><ul><li><strong>缺乏序列建模</strong>： <ul><li>在自然語言處理（NLP）任務中，理解詞語的順序及其依賴關係對準確的預測至關重要。</li></ul></li><li><strong>固定輸入大小</strong>： <ul><li>FFN 需要固定大小的輸入，這在 NLP 任務中會產生問題，因為輸入序列的長度通常是變化的。</li></ul></li><li><strong>有限的上下文信息</strong>： <ul><li>許多 NLP 任務需要捕捉長距依賴性和理解文本的廣泛上下文，而這些更適合能夠有效建模序列數據的模型。</li></ul></li></ul><h2 id="rnn" tabindex="-1"><a class="header-anchor" href="#rnn"><span>RNN</span></a></h2><h3 id="遞歸神經網絡-rnn" tabindex="-1"><a class="header-anchor" href="#遞歸神經網絡-rnn"><span>遞歸神經網絡（RNN）</span></a></h3><ul><li>在時間 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 的方程式如下：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>V</mi><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y_t = g(Vh_t) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>U</mi><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi>W</mi><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = f(Ux_t + Wh_{t-1}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p></li><li>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span> 是激活函數。</li></ul><figure><img src="data:image/png;base64,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" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="rnn-的特性" tabindex="-1"><a class="header-anchor" href="#rnn-的特性"><span>RNN 的特性</span></a></h3><ul><li><strong>序列處理</strong>： <ul><li>RNN 能處理序列數據，允許它們模擬數據中的時間依賴性。</li></ul></li><li><strong>遞歸連接</strong>： <ul><li>RNN 維持內部記憶，有助於捕捉長期依賴。</li></ul></li><li><strong>參數共享</strong>： <ul><li>RNN 在不同時間共享參數，增強了學習序列數據的效率。</li></ul></li><li><strong>梯度消失問題</strong>： <ul><li>傳統的 RNN 可能會遇到梯度消失的問題，這阻礙了長期依賴的學習。</li></ul></li></ul><h2 id="rnn-應用" tabindex="-1"><a class="header-anchor" href="#rnn-應用"><span>RNN 應用</span></a></h2><h3 id="rnn-應用於命名實體識別-ner" tabindex="-1"><a class="header-anchor" href="#rnn-應用於命名實體識別-ner"><span>RNN 應用於命名實體識別（NER）</span></a></h3><ul><li>在標記分類任務中，每個輸出應該映射到一個 one-hot 向量。</li><li>在 RNN 模型中增加了一個前饋網絡。</li></ul><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="rnn-應用於序列分類" tabindex="-1"><a class="header-anchor" href="#rnn-應用於序列分類"><span>RNN 應用於序列分類</span></a></h3><ul><li>RNN 用於對整個序列進行分類，而不是序列中的單個標記。</li><li>取文本最後一個標記的隱藏層。</li></ul><figure><img src="data:image/png;base64,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" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="堆疊式-rnns" tabindex="-1"><a class="header-anchor" href="#堆疊式-rnns"><span>堆疊式 RNNs</span></a></h3><ul><li>堆疊式 RNNs 由多個網絡組成，其中一層的輸出作為後續層的輸入。</li></ul><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><strong>堆疊式 RNNs 通常比單層網絡表現更好</strong>： <ul><li>網絡在不同層次上誘導不同程度的抽象表示。</li><li>堆疊網絡的初始層會產生有用的抽象表示，這些表示可以為後續層提供支持。</li></ul></li><li><strong>然而，隨著堆疊層數增加，訓練成本也會快速上升</strong>。</li></ul><h3 id="雙向-rnns" tabindex="-1"><a class="header-anchor" href="#雙向-rnns"><span>雙向 RNNs</span></a></h3><ul><li><strong>在許多應用中，RNN 需要訪問整個輸入序列</strong>。</li><li><strong>引入了雙向 RNN</strong>： <ul><li>它結合了兩個獨立的雙向 RNN，一個從頭到尾處理輸入，另一個則從尾到頭處理。</li></ul></li></ul><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="用於序列分類的雙向-rnns" tabindex="-1"><a class="header-anchor" href="#用於序列分類的雙向-rnns"><span>用於序列分類的雙向 RNNs</span></a></h3><ul><li><strong>來自前向和後向傳遞的最終隱藏單元結合在一起，表示整個序列</strong>。</li><li><strong>這個結合的表示作為後續分類器的輸入</strong>。</li></ul><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="總結" tabindex="-1"><a class="header-anchor" href="#總結"><span>總結</span></a></h2><ul><li><strong>語言模型（LM）</strong>：為詞語序列賦予概率的模型。</li><li><strong>N-gram 語言模型（LM）</strong>：傳統的統計語言模型。</li></ul><h3 id="嵌入方法" tabindex="-1"><a class="header-anchor" href="#嵌入方法"><span>嵌入方法：</span></a></h3><ul><li><strong>稀疏向量（Sparse vectors）</strong>：使用上下文來編碼詞嵌入。（如 TF-IDF, PPMI）</li><li><strong>稠密向量（Dense vectors）</strong>：應用自監督學習來生成。（如 Word2Vec, Contextualized Embeddings）</li></ul><h3 id="神經語言模型-neural-lms" tabindex="-1"><a class="header-anchor" href="#神經語言模型-neural-lms"><span>神經語言模型（Neural LMs）：</span></a></h3><ul><li><strong>FFN</strong>：全連接的稠密神經網絡。</li><li><strong>RNN</strong>：循環神經網絡，儲存時間序列的隱藏狀態。</li></ul>',96)),(0,t.Q3)(' ## 總結 \n\n### How\n\n把文章切句子，找詞性 (主詞 名詞 動詞) by 語言學，可以理解句子\n對於古老方式這很難，不過現在有 GPT 那些的\n\n\n\n### Word sense ambiguity \n\n有多種意思的話，怎麼理解? \n\nex: Watch for kids ? \n是小孩手錶 or 注意小孩活動告示? \n\n- 很有趣 back Translation \n    ex:  把中文翻英文 再把英文翻中文 大概有一半會不一樣\n    \n## 舊方法\n\n用一些方法找出關鍵字，然後分類 (example: 好/壞)。\n\n![](https://hackmd.io/_uploads/r1RzDmepA.png)\n\n像這張圖，那如果出現了紅色的 "新鮮" 和 "價位低"，要怎麼知道是好還是壞呢? 又或者是罕見詞出現呢？\n\n希望比較彈性一點\n\n### One-hot encoding \n\n\n![](https://hackmd.io/_uploads/H1i-umla0.png)\n\n壞處: \n- 必須要同樣字才行\n- 向量太大了，效率很有問題\n\n但以前沒辦法只能這樣做\n\n### Word embedding(dense space)\n\n(2000~2014) 左右。\n\n所以需要壓縮一下空間。 使向量短，且彼此間可能有所關連。\n\n![](https://hackmd.io/_uploads/HysBuQgaR.png)\n\n使之後如果遇到比較不一樣的字詞，可以評估到底是哪一種分類。\n\n### Synonymy and Polysemy\n\n因為語言有\n\nSynonymy (同義詞)\n\nPolysemy (一詞多義)\n\n可能會產生模糊的狀況\n\n### Concept matching v.s. term matching \n\n不同領域間會使用不同的字詞\n(計程車，出租車)\n\n(智慧型行動裝置，手機)\n\n或者是專有名詞 ex (COVID-19，新冠病毒，武漢肺炎)\n\n### How do we represent the meaning of a word \n\nwordnet 有建立一些資料集合\n\n![](https://hackmd.io/_uploads/HybAKmgpR.png)\n![](https://hackmd.io/_uploads/Bk5ec7lTR.png)\n\n但後來發現建立不完，所以其實也不太好用。\n\n### Continuous, Distributed \n\n![](https://hackmd.io/_uploads/rkxHcmxTR.png)\n\n\n\n### Language Model\n\n(Andrey Markov, Claude Shannon)\n: 大語言模型是一個接龍遊戲\n\nGPT 只知道接龍，不知道語意 (但其實不太對)\n\n轉成 vector space 比較 dense 就可算出彼此間的相關性，不須處理比較長的 vector\n\nex: RAG LLM \n\n但也造成了他學會了一些歧視 (例如 男生醫生 女生護士)。\n\n![](https://hackmd.io/_uploads/SkJrnXx6A.png)\n\n\n### Distributional Hypothesis\n\n相似的詞會在類似的地方\n\nex: The cat licked its fur \nThe dog licked its fur\n\n但不會出現 eat run bite 或者 wheel truck \n\n大型語言就是這樣靠自我監督，慢慢把彼此間關聯。\n\n其實之前 NLP 的時候就有了。 1990 \n\n- 有出現不見得是相關\n- 沒出現不見得是相關\n\nLSI 用統計方式，再用 SVD 把重要成份取出來，但現在這個方法不好。\n\n- 矩陣很大 ex: 100000\\*100000 跑 SVD 很麻煩\n- 運算量太高，不太適用\n- 不過感覺很厲害，在那時候是很厲害的一種想法\n\n### Probabilistic Model\n\n![](https://hackmd.io/_uploads/By0AyVlTR.png)\n\n用條件機率的方式來做處理\n\n> 感覺有點類似馬可夫過程\n\n### Model Parameters \n\n![](https://hackmd.io/_uploads/r1bvXVeaC.png)\n\n\n- 有個前提，如果一個類神經網路，可以透過加加減減得到後面的文字。懂得文字的意思，那中間的幾百個數字是不是能很清楚代表文字的意思？\n\n很像 GPT 背了一堆書，你給他開頭，他回答了一串文字，那這樣他是不懂嗎？ 好像也說不過去，但你說他懂，但他好像只是會背書。\n\n### What vectors should I use -- it depends \n\n![](https://hackmd.io/_uploads/B14tL4e6A.png)\n\n\n ')])}]]),d=JSON.parse('{"path":"/Posts/NLP/NLP3.html","title":"NLP word embedding and RNN","lang":"en-US","frontmatter":{"title":"NLP word embedding and RNN","order":3,"date":"2024-10-04T00:00:00.000Z","category":["NLP"],"tag":["note","theorem"],"description":"高宏宇 - 教授 N-grams N-gram 是一個包含 n 個單詞的序列。通過不同數量的單詞組合，N-gram 模型可以捕捉語言中的上下文關係。 1-gram（unigram）：單詞序列中的每個單詞單獨作為一個項目，例如： \\"please\\" \\"turn\\" \\"your\\" \\"homework\\" 2-gram（bigram）：兩個連續的單詞組合，例如： ...","head":[["meta",{"property":"og:url","content":"https://hippotumux.github.io/blog/blog/Posts/NLP/NLP3.html"}],["meta",{"property":"og:site_name","content":"Hippotumux\'s Blog"}],["meta",{"property":"og:title","content":"NLP word embedding and RNN"}],["meta",{"property":"og:description","content":"高宏宇 - 教授 N-grams N-gram 是一個包含 n 個單詞的序列。通過不同數量的單詞組合，N-gram 模型可以捕捉語言中的上下文關係。 1-gram（unigram）：單詞序列中的每個單詞單獨作為一個項目，例如： \\"please\\" \\"turn\\" \\"your\\" \\"homework\\" 2-gram（bigram）：兩個連續的單詞組合，例如： ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://hackmd.io/_uploads/r1RzDmepA.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-02-24T03:41:58.000Z"}],["meta",{"property":"article:tag","content":"note"}],["meta",{"property":"article:tag","content":"theorem"}],["meta",{"property":"article:published_time","content":"2024-10-04T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-02-24T03:41:58.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"NLP word embedding and RNN\\",\\"image\\":[\\"https://hackmd.io/_uploads/r1RzDmepA.png\\",\\"https://hackmd.io/_uploads/H1i-umla0.png\\",\\"https://hackmd.io/_uploads/HysBuQgaR.png\\",\\"https://hackmd.io/_uploads/HybAKmgpR.png\\",\\"https://hackmd.io/_uploads/Bk5ec7lTR.png\\",\\"https://hackmd.io/_uploads/rkxHcmxTR.png\\",\\"https://hackmd.io/_uploads/SkJrnXx6A.png\\",\\"https://hackmd.io/_uploads/By0AyVlTR.png\\",\\"https://hackmd.io/_uploads/r1bvXVeaC.png\\",\\"https://hackmd.io/_uploads/B14tL4e6A.png\\"],\\"datePublished\\":\\"2024-10-04T00:00:00.000Z\\",\\"dateModified\\":\\"2025-02-24T03:41:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Hippotumux\\",\\"url\\":\\"https://hippotumux.github.io/blog/\\"}]}"]]},"headers":[{"level":2,"title":"N-grams","slug":"n-grams","link":"#n-grams","children":[{"level":3,"title":"N-gram 語言模型","slug":"n-gram-語言模型","link":"#n-gram-語言模型","children":[]},{"level":3,"title":"Bi-gram 模型示例","slug":"bi-gram-模型示例","link":"#bi-gram-模型示例","children":[]},{"level":3,"title":"模型比較","slug":"模型比較","link":"#模型比較","children":[]}]},{"level":2,"title":"困惑度（Perplexity, PPL）","slug":"困惑度-perplexity-ppl","link":"#困惑度-perplexity-ppl","children":[{"level":3,"title":"意義","slug":"意義","link":"#意義","children":[]}]},{"level":2,"title":"Word Embedding","slug":"word-embedding","link":"#word-embedding","children":[{"level":3,"title":"嵌入的特性","slug":"嵌入的特性","link":"#嵌入的特性","children":[]}]},{"level":2,"title":"Word2Vec","slug":"word2vec","link":"#word2vec","children":[{"level":3,"title":"Word2Vec 流程","slug":"word2vec-流程","link":"#word2vec-流程","children":[]},{"level":3,"title":"上下文嵌入（Contextualized Word Embeddings）","slug":"上下文嵌入-contextualized-word-embeddings","link":"#上下文嵌入-contextualized-word-embeddings","children":[]},{"level":3,"title":"神經網絡架構中的嵌入層和語言模型層","slug":"神經網絡架構中的嵌入層和語言模型層","link":"#神經網絡架構中的嵌入層和語言模型層","children":[]}]},{"level":2,"title":"神經語言模型","slug":"神經語言模型","link":"#神經語言模型","children":[{"level":3,"title":"深度學習基礎","slug":"深度學習基礎","link":"#深度學習基礎","children":[]},{"level":3,"title":"訓練神經網絡","slug":"訓練神經網絡","link":"#訓練神經網絡","children":[]},{"level":3,"title":"激活函數","slug":"激活函數","link":"#激活函數","children":[]}]},{"level":2,"title":"FFN","slug":"ffn","link":"#ffn","children":[{"level":3,"title":"前饋神經網絡（FFN）","slug":"前饋神經網絡-ffn","link":"#前饋神經網絡-ffn","children":[]},{"level":3,"title":"前饋神經網絡（FFN）的缺點","slug":"前饋神經網絡-ffn-的缺點","link":"#前饋神經網絡-ffn-的缺點","children":[]}]},{"level":2,"title":"RNN","slug":"rnn","link":"#rnn","children":[{"level":3,"title":"遞歸神經網絡（RNN）","slug":"遞歸神經網絡-rnn","link":"#遞歸神經網絡-rnn","children":[]},{"level":3,"title":"RNN 的特性","slug":"rnn-的特性","link":"#rnn-的特性","children":[]}]},{"level":2,"title":"RNN 應用","slug":"rnn-應用","link":"#rnn-應用","children":[{"level":3,"title":"RNN 應用於命名實體識別（NER）","slug":"rnn-應用於命名實體識別-ner","link":"#rnn-應用於命名實體識別-ner","children":[]},{"level":3,"title":"RNN 應用於序列分類","slug":"rnn-應用於序列分類","link":"#rnn-應用於序列分類","children":[]},{"level":3,"title":"堆疊式 RNNs","slug":"堆疊式-rnns","link":"#堆疊式-rnns","children":[]},{"level":3,"title":"雙向 RNNs","slug":"雙向-rnns","link":"#雙向-rnns","children":[]},{"level":3,"title":"用於序列分類的雙向 RNNs","slug":"用於序列分類的雙向-rnns","link":"#用於序列分類的雙向-rnns","children":[]}]},{"level":2,"title":"總結","slug":"總結","link":"#總結","children":[{"level":3,"title":"嵌入方法：","slug":"嵌入方法","link":"#嵌入方法","children":[]},{"level":3,"title":"神經語言模型（Neural LMs）：","slug":"神經語言模型-neural-lms","link":"#神經語言模型-neural-lms","children":[]}]}],"git":{"createdTime":1740329256000,"updatedTime":1740368518000,"contributors":[{"name":"hippotumux","username":"hippotumux","email":"st6805972@gmail.com","commits":2,"url":"https://github.com/hippotumux"}]},"readingTime":{"minutes":12.16,"words":3649},"filePathRelative":"Posts/NLP/NLP3.md","localizedDate":"October 4, 2024","excerpt":"\\n<p>高宏宇 - 教授</p>\\n<h2>N-grams</h2>\\n<p><strong>N-gram</strong> 是一個包含 n 個單詞的序列。通過不同數量的單詞組合，N-gram 模型可以捕捉語言中的上下文關係。</p>\\n<ul>\\n<li><strong>1-gram（unigram）</strong>：單詞序列中的每個單詞單獨作為一個項目，例如：\\n<ul>\\n<li>\\"please\\"</li>\\n<li>\\"turn\\"</li>\\n<li>\\"your\\"</li>\\n<li>\\"homework\\"</li>\\n</ul>\\n</li>\\n<li><strong>2-gram（bigram）</strong>：兩個連續的單詞組合，例如：\\n<ul>\\n<li>\\"please turn\\"</li>\\n<li>\\"turn your\\"</li>\\n<li>\\"your homework\\"</li>\\n</ul>\\n</li>\\n<li><strong>3-gram（trigram）</strong>：三個連續的單詞組合，例如：\\n<ul>\\n<li>\\"please turn your\\"</li>\\n<li>\\"turn your homework\\"</li>\\n</ul>\\n</li>\\n<li><strong>N-gram</strong> 模型可擴展至更高的數值，例如 4-gram、n-gram 等。</li>\\n</ul>\\n","autoDesc":true}')}}]);